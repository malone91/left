端口是什么？
用来区分通信的时候知道是哪个服务


单线程处理Socket
每个请求一个线程
固定大小线程池处理
 服务员线程

单线程处理Socket
每个请求一个线程
固定大小线程池处理

存在两种类型操作
CPU计算、业务处理
IO操作与等待/网络、磁盘、数据库

Socket通信模型
建立连接
开始通信
结束通信

建立服务端通信Socket
等待并接收连接请求

创建连接Socket并向服务端发起请求

接收请求后创建连接Socket

InputStream OutputStream

关闭Socket相关资源（服务端和客户端都有）

为什么创建比CPU核心数多的线程？
有Sleep操作，等待处理，不消耗CPU
加入创建的线程数只有核心的CPU数的话，那么线程是吃不饱的，CPU占用被释放了，没有别的线程来
接，这样CPU资源就浪费掉了。不是所有的线程都在忙或者干活，有些员工摸鱼，所以有更多的员工干事。

对于一个IO相关应用来说，通过网络访问读取本地文件再返回给客户端，
大部分CPU使用的非常少，等资源可能就被浪费了。
分工协调上下文切换，所以需要统筹，
GC并发收集的时候，不影响业务线程 并行处理的多一些

不仅面临CPU、线程的问题，还有数据来回复制的问题
用户空间、内核空间

线程的等待导致CPU使用率不高、创建更多的线程导致了竞争，底层Socket操作更复杂，有很多资源消耗，这
就涉及到IO的模型，通信模型

阻塞、非阻塞 是线程处理模式
同步、异步 是通信模式 线程要不要拦住

		阻塞		非阻塞
同步   阻塞IO模型   非阻塞IO模型
       IO复用模型   信号驱动IO模型
异步                异步IO模型

基本上都是同步的

发起者的线程和处理结果的线程不是一个线程
叫人去买东西，自己做自己的事情，同步非阻塞。两个线程在做，把结果告诉我，不是当前线程做，背后有异步的线程池去做
在发起的线程看来是同步操作，但是比以前快了，有其他线程在帮助做，future.get()

IO模型 01- 阻塞式BIO IO
一般通过在while(true)循环中服务端会调用accept()方法等待接受客户端的连接的方式监听请求，请求一旦接收到一个连接请求，
就可以建立通信套接字并在这个通信套接字上进行读写操作，此时不能再接收其他客户端连接请求，只能等待同当前连接的客户端的操作执行
完成，不过可以通过多线程来支持多个客户端的链接。
线程跟内核打交道，不是直接和网络
IO模型 02- 非阻塞IO
和阻塞IO类比内核会立即返回，返回后获得足够的CPU时间继续做其他的事情，用户进程第一个阶段不是阻塞的，
需要不断的主动轮询kernel数据好了没有，第二个阶段依然总是阻塞的
多了个看有没有准备好，其他的还是和阻塞没什么区别
IO模型 03- 多路复用
IO multiplexing 时间驱动，就是在单个线程里同时监控多个套接字，通过select或poll轮询所负责的所有socket，
当某个socket有数据到达了，就通知用户进程。
IO复用同非阻塞IO本质一样，不过利用了新的select系统调用，由内核来负责本来是请求进程该做的轮询操作。
看似比非阻塞IO还多了一个系统调用开销，不过因为可以支持多路IO，才提高了效率。
进程先是阻塞在select/poll 上，再是阻塞在读操作的第二个阶段上。

select和poll的几大缺点
1、每次调用select都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大
2、同时每次调用select都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大
3、select支持的文件描述符数量太小了，默认1024
还是性能不够高
select数组，poll链表

epoll react模型
1、内核与用户空间进程共享一块内存的buffer区，避免了来回复制的问题
2、通过回调解决遍历问题，红黑树，循环递归查找点的效率提高了，注册回调的机制 返回准备好了的
3、fd没有限制，可以支撑10万连接
不需要长期等待的过程

IO模型 04- 信号驱动IO  用的比较少
信号驱动IO与BIO/NIO最大的区别就在于，在IO执行的数据准备阶段，不会阻塞用户进程
如Linux网络编程的图，当用户进程需要等待数据的时候，会向内核发送一个信号，告诉内核我需要什么数据，然后用户进程就继续做别的事情了，
而当内核中的数据准备好之后，内核立马发给用户进程一个信号，说数据准备好了，用户进程收到信号之后，立马调用recvfrom区接收数据

01:25-01:40视频时间
线程池-->EDA-->SEDA分阶段事件驱动架构，多个事件驱动加线程池
线程池确缺点：并发能力有一个峰值，到了一个最佳值后系统吞吐量会减小，延迟会上升
事件驱动：异步处理、回调，发送事件，提高吞吐量
事件处理机制：更为好，把发起的线程变成一个事件传出去，真正处理业务的这个模块或者线程，处理完后会继续 系统更为平稳
发起一个新的事件，链表？发起者没有直接调用 bus ，guava中有一个类，EventBus
发起者接收到任务处理完的事件继续走，跟异步是有区别的
事件处理机制的事件拆分的特别碎，没有关联，只需要知道从哪里拿事件；
发起者和最后一步可以在一个线程中做，可控性更强

异步最后一步是在被调起方做

IO模型 05-异步式IO  Proactor
异步IO 真正实现了IO全流程的非阻塞。用户进程发出系统调用后立即返回，内核等待数据准备完成，然后将数据拷贝到
用户进程缓冲区，然后发送信号告诉用户进程IO操作执行完成
与SIGIO相比，一个是发送信号告诉用户进程数据准备完毕，一个是IO执行完毕 Windows的IOCP模型

epoll是Reactor

这几种模式发明的时间比较早

全流程处理IO的链给切割开
流式处理

netty支持NIO BIO Linux-epoll Windows-iocp 支持TCP HTTP UDP，做了通用的壳子，切换不同的组件
并发不大，BIO厉害

异步JDBC nodejs异步代码非常多

httpserver：netty springboot 搞出的server 无状态的
webserver：Tomcat web容器环境有要求：支持session，jsp
jboss：数据库连接放在了Jboss服务器的配置上
jsp就是servlet，例子代码叫service方法，jsp生成servlet文件，然后编译为class文件

手写网关

netty是一个异步的网络框架
websocket是二进制的

netty特性：高性能的协议服务器，高吞吐，低延迟，低开销，零拷贝，可扩容，松耦合：网络和业务逻辑分离，使用方便、可维护性好

mina
一些新的协议是基于UDP做的，变得可靠，比如QQ 交易所提供的api是udp，效率高

越偏底层的代码越不讲究
越通用越平庸
自定义参数选择用哪个，去平衡trade-off

netty把底层的网络细节屏蔽掉，netty提供了参数供调整

netty的应用：嵌入式：http server，HTTPSserver， websocketserver，tcp server ,udp server, in vm pipe

k线图 基本上是websocket

订单处理，流式处理

什么是高性能？
高并发用户、高吞吐量、低延迟
外部，业务指标；后两个是技术指标

wrk -c 40 -d 30s --latency url
延迟和响应时间RT的区别？ t1发，t2到
延迟是针对系统内的指标，请求进入系统时间t3，请求出系统时间t4,t4-t3就是系统的延迟
响应时间是针对客户或调用者的
网络条件不好区别很大，条件好没区别。

延迟分布
    50% 1.57ms P50
    75% 2.07ms P75
    90% 2.64ms P90
    99% 4.05ms P99 之内

高性能缺点：系统复杂度高，建设与维护成本，故障或bug的破坏性大
应对策略：稳定性建设，混沌工程，
容量，爆炸半径，工程方面积累与改进

容量：TPS当前市场最大：
一天有86400。淘宝一天3000万订单 京东一两千万
三四百的TPS
支付宝双十一TPS最高45万左右，国内最大的并发
双十一搞5-10倍数量的机器
爆炸半径：70%的故障是变动引起的，比如代码，数据库，环境，能不能有些办法控制这些范围。一次相关上线的影响范围
缩小到一个模块内，其他模块不会受影响。微服务也是把爆炸半径缩小了
工程方面积累与改进：天灾，人祸

netty怎么实现高性能？
网络开发框架:异步，事件驱动，基于NIO
适用于：服务端，客户端，TCP/UDP

从事件处理机制
Event Mediator 事件分发器 取号器，等待服务，EventChannel相当于桌子，被提供服务
要过来吃放的人，成为真正的顾客

到React模型
Web服务器把输入的各种请求hold住，再分发给其他的服务员，进行服务

流水线加Handler方式最大化能够有有更多的灵活性，不同协议或报文处理加不同的handler
老板少工人多 worker/boss group 线程池
EventLoop是一个不断轮询的单线程的处理器，单线程没有上下文切换和资源的浪费 线程

粘包与拆包在应用层，上层

MTU 最大传输单元 1500Byte默认
MSS 最大分段大小 1460Byte  少了40，TCP头和IP的头分别占用了20字节

网络拥堵与Nagle算法优化 TCP_NODELAY
Nagle算法：发送数据 的缓冲区满，达到超时200ms，把数据都丢给网络发出去
Socket变成，调用send和receive时并不是真正的发或者收 Nagle算法决定是不是合并到一块，那么会有小的包延迟比较高
优化条件：缓冲区满，达到超时200ms

操作系统会优化，所以我们可以把这个算法丢掉 延迟敏感 可以直接调TCP的参数

http的断点续传

1.6k数据性能比1k差很多 尽量卡到MSS

还有一个连接优化：端口占用的问题

三次握手：在吗？ 我在，你在不在？ 在 然后建立了连接
谁接到对方的ack，就可以把自己设置为建立连接
假如第三次握手没收到怎么办？不怎么办，对客户端来说已经建立了，继续发消息，会超时报错，出现在这还好。

TCP连接必须经过时间2MSL后才真正释放掉
四次挥手：客户端的 TIME_WAIT 状态 客户端本地要占用端口 等待2个MSL 时长，Linux2min，Windows1min 也就是4/2分钟，然后才把TIME_WAIT关闭

TIME_WAIT就是空窗期

windows命令 netstat -anot  有好多TIME_WAIT 等几分钟才关闭 再启动，端口占用会失败

1、降低MSL 降低Socket的回收周期
2、端口打开复用的参数 在没有完全释放的时候可以直接通用掉

netty优化
1、不要阻塞EventLoop
2、系统参数优化
ulimit -a /proc/sys/net/ipv4/tcp_fin_timeout,TcpTimedWaitDelay 降低Socket的回收周期，Linux和Windows上的参数
1025--->可以调整到6万多 一切都是文件

3、缓冲区优化
SO_RCVBUF SO_SNDBUF
SO_BACKLOG 真正通信的连接的个数
SO_REUSEADDR 重用绑定的端口和地址

4、心跳周期优化
心跳机制与断线重连 Socket连接断了，心跳然后重新连接  和数据连接池相关 发个空包或一个sql select 1  jdbd有一个更方便的isValid
5、内存与ByteBuffer优化
DirectBuffer与HeapBuffer 把真实的物理内存映射到内存里 不用来回写，指定使用堆外原生内存 业务用堆内存 Netty的native memory自己反复使用，不用再申请内存
减少来回的拷贝。
6、其他优化
ioRatio 50:50
watermark 水位 缓冲区相关， 是一个配置参数
trafficShaping流量整形 是一个handler，配置到pipeline中

Lua连接Redis  ---->业务网关

springcloud gateway基于netty

最简单的网关就是个代理，接收请求，代替去请求后端服务
然后中间加过滤器，就是handler，改变http的头，改变返回数据的报文信息
然后中间加router，路由，负载均衡

httpclient换成netty的client

重复利用内存，效率比较高，不用重新申请内存
.option(ChannelOption.ALLOCATOR, PooledByteBufAllocator.DEFAULT);

猫大人网关

######
单线程最怕卡死，IO和业务处理分开 和线程池分离 http的请求服务的时候，线程池没有在当前线程中写

BECH
网关代码：filter inbound outbound router application

分布式事务，都是靠时钟来保证的