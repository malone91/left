Netty对Reactor的支持 源码角度

Netty如何支持主从Reactor模式的？
为什么Netty的main reactor大多不能用到一个线程组，只能线程组里的一个
Netty给Channel分配nio loop event 的规则是什么
通用模式的nio实现多路复用器怎么跨平台的

bossGroup是main reactor
workerGroup是sub reactor
group.register(channel) channel指的是ServerSocketChannel
将ServerSocketChannel绑定到了bossGroup

group(bossGroup, workerGroup);
源码查看方式：构造方法，得到group，Alt+F7看谁read了这个方法，有个group()方法
然后看group方法的调用，鼠标放到方法上，ctrl+Alt+H，找到initAndRegister方法
ChannelFuture regFuture = config().group().register(channel);
拿到了group，将channel绑定到了bossGroup （ServerSocketChannel）
ServerSocketChannel就可以创建子的socketChannel，绑定到创建的workerGroup上
ChannelRead

两种socketChannel绑定到两个group中，完成了主从reactor的模式

跨平台实现源码

给channel分配NioEventLoop的时候，根据不同的情况选择不同的算法

粘包半包概念 ABC DEF ABCDEF , AB CD EF

为什么TCP会出现
发送方每次写入数据<套接字缓冲区大小  每次写入数据小，网卡不会立马发送，合并发送，粘包
接收方读取套接字缓冲区数据不够及时 对方读取数据不及时，发送方发送数据大于MTU，必须拆包，发多次， 是个半包

收发角度:一个发送可能多次接收，多个发送可能一次接收
传输角度：一个发送可能占用多个传输包，多个发送可能公用一个传输包
TCP是流式协议，消息无边界
UDP没有粘包半包 接收一个一个接收

解决方案：
固定字段存储内容长度信息   固定长度浪费空间，分隔符需要扫描内容

常用解决方法
Netty对三种常用封帧方式的支持:封装成帧
固定长度：FixedLengthFrameDecoder
分隔符：DelimiterBasedFrameDecoder
固定长度字段存内容长度信息： LengthFieldBasedFrameDecoder

Netty处理粘包半包的源码
解码核心工作流程？
解码中两种数据积累器Cumulator的区别？
三种解码器的常用的额外控制参数有哪些？

三种解码器都继承于 ByteToMessageDecoder

Netty对一些编解码的支持，常用二次编解码方式，

为什么要二次编码
把解决半包粘包问题的常用三种解码器叫一次解码器，一次解码的结果是字节数组
需要和项目中使用的对象做转化，方便使用，这层解码器成为二次解码，相应的编码器是把Java对象转化成字节流方便存储或运输

一次解码器继承 ByteToMessageDecoder 读，解码
 io.netty.buffer.ByteBuf(原始数据流)，可能出现半包粘包问题的数据流---->io.netty.buff.ByteBuf(用户数据，还是字节数组)
二次解码器继承 MessageToMessageDecoder<I> 读，解码
 io.netty.buff.ByteBuf(用户数据) -----> Java Object 能很好的使用

分层思想 天下大同，天下归一

常用的二次编解码方式：Java序列化（很少用，占空大） Marshaling XML JSON MessageBack  Protobuf 其他

Google Protobuf 可读性差，性能好 跨语言，自带编译器
编解码选型依据：压缩后空间大小

Netty序列化serialization把元信息省略了 魔数也省略了 其实还是可以work的，用反射把信息读取出来

Netty是怎么支持 使用protobuf的 两对编解码 WorldClock example

keepalive参数  idle监测
为什么需要keepalive 怎么设计？以TCP层为例
为什么需要应用层keepalive idle监测是什么
如何在Netty中开启TCP keepalive 和idle监测

keepalive：问还在不，是否继续聊

需要处理keepalive的场景：对端数据异常，对端处理不过来，对端不可达
不做keepalive的后果：连接已坏，还要浪费资源维持，下次调用直接报错

uname -a
uname -r
cat /proc/version
cat /proc/sys/net/ipv4/tcp_keepalive_time
如何设计keepalive：出现概率小，没有必要频繁，判断要谨慎
#sysctl -a | grep tcp_keepalive
net.ipv4.tcp_keepalive_time = 7200 s 距离上次传送数据多少时间未收到新报文判断为开始检测
net.ipv4.tcp_keepalive_intvl = 75 s 下一个探测宝的探测时间
net.ipv4.tcp_keepalive_probes = 9 s
tcp_keepalive默认关闭；打开时，TCP在连接没有数据通过的7200秒后发送keepalive信息，当探测没有确认时，按75
秒的重试频率重发，一直发9个探测包都没有确认，就认定连接失效
所有总耗时为 7200+75*9 = 2h 11min

为什么需要应用层keepalive？ 分层 分层 分层
协议分层，关注点不同，传输层关注通，应用层关注是否可用；电话打通不代表有人接，服务器连接在，不一定可以服务
TCP层keepalive默认关闭，经过路由中转设备keepalive包可能被丢弃
TCP keepalive时间长，虽然可改，但属于系统默认参数，改动影响所有应用

HTTP keepalive指的是对长连接和短连接的选择
HTTP1.1默认长连接 Connection：keep-Alive 不需要带这个header，抓包可能看不到
Connection：close 代表短连接

idle监测是什么？
对方在一定时间内不说话，认定对方存在问题idle，于是开始发问，keepalive你还在么 或者不问直接断开连接
idle只是负责诊断，诊断后作出不同的行为，决定idle监测的最终意义
1、配合发送keepalive，减少keepalive的消息
 keepalive设计演进：
 V1定时keepalive消息--->V2空闲监测+判定为Idle时才发keepalive
2、直接关闭连接
快速释放损坏的恶意的很久不用的连接，让系统时刻保持最好的状态
简单粗暴，客户端可能需要重连

实际使用：按需keepalive，保证不会空闲，如果空闲关闭连接

Netty源码层面支持tcp_keepalive_time 和idle监测的介绍：
server端开启TCP keepalive
boostrap.childOption(ChannelOption.SO_KEEPALIVE, true)
boostrap.childOption(NioChannelOption.of(StandardSocketOption.SO_KEEPALIVE), true)
.option(ChannelOption.SO_KEEPALIVE, true)存在但是无效

开启idle check
ch.pipeline().addLast("idleCheckHandler", new IdleStateHandler(0, 20, 0, TimeUnit.SECONDS));
                              int readerIdleTimeSeconds, int writerIdleTimeSeconds, int allIdleTimeSeconds
0表示关闭

回到echoserver例子
看bootstrap.childOption方法 child是跟客户端连接的SocketChannel 底层调用jdk，规避jdk bug

IdleStateHandler.hasOutputChange 看是否有写的意图，而不是写成功


ReadTimeOutHandler
WriteTimeOutHandler

fireExceptionCaught