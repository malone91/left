基本就是Spring源码大体原型了，委托的BeanFactory在Spring源码里是DefaultListableBeanFactory。循环依赖解决是三级缓存，提前暴露还没有初始化结束的bean。
检测是Map存一下过程，aba这样顺序判断，有重复（a出现两次）就是环了。

三级缓存源码对应
org.springframework.beans.factory.support.DefaultSingletonBeanRegistry#getSingleton

/**
* Return the (raw) singleton object registered under the given name.
* <p>Checks already instantiated singletons and also allows for an early
* reference to a currently created singleton (resolving a circular reference).
* @param beanName the name of the bean to look for
* @param allowEarlyReference whether early references should be created or not
* @return the registered singleton object, or {@code null} if none found
*/
@Nullable
protected Object getSingleton(String beanName, boolean allowEarlyReference) {
Object singletonObject = this.singletonObjects.get(beanName);
if (singletonObject == null && isSingletonCurrentlyInCreation(beanName)) {
synchronized (this.singletonObjects) {
singletonObject = this.earlySingletonObjects.get(beanName);
if (singletonObject == null && allowEarlyReference) {
ObjectFactory<?> singletonFactory = this.singletonFactories.get(beanName);
if (singletonFactory != null) {
singletonObject = singletonFactory.getObject();
this.earlySingletonObjects.put(beanName, singletonObject);
this.singletonFactories.remove(beanName);
}
}
}
}
return singletonObject;
}


/** Cache of singleton objects: bean name to bean instance. */
private final Map<String, Object> singletonObjects = new ConcurrentHashMap<>(256);

/** Cache of singleton factories: bean name to ObjectFactory. */
private final Map<String, ObjectFactory<?>> singletonFactories = new HashMap<>(16);

/** Cache of early singleton objects: bean name to bean instance. */
private final Map<String, Object> earlySingletonObjects = new HashMap<>(16);

assertEquals(Short.valueOf("1"), converter.convert("1"));
        assertNull(converter.convert(null));
        assertThrows(NumberFormatException.class, () -> {
            converter.convert("ttt");
        });
		
 java -XX:+PrintCommandLineFlags -version
 
 wc -l gc.log
 wc -w gc.log
 jmap -dump:format=b,file=E:\heap.hprof 8864
 GC (Allocation Failure)  minor GC
 java -Dapp.home=/app/weblogic/scs -server -Xms4G -Xmx4G -Xmn2g -Xss256k -XX:PermSize=128m -XX:MaxPermSize=512m -Djava.awt.headless=true
 -Dfile.encoding=utf-8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:AutoBoxCacheMax=20000
  -XX:-OmitStackTraceInFastThrow -XX:ErrorFile=/app/weblogic/scs/logs/hs_err_%p.log -XX:+HeapDumpOnOutOfMemoryError
  -XX:HeapDumpPath=/app/weblogic/scs/logs/ -Xloggc:/app/weblogic/scs/logs/gc.log -XX:+PrintGCDetails -XX:+PrintGCDateStamps -jar /app/weblogic/scs/webapps/scs-service-0.0.1-SNAPSHOT.jar 810
 
 借助命令模式，我们可以将函数封装成对象。
 
 Hasen 模型里面，要求 notify() 放在代码的最后，这样 T2 通知完 T1 后，T2 就结束了，然后 T1 再执行，这样就能保证同一时刻只有一个线程执行。
 Hoare 模型里面，T2 通知完 T1 后，T2 阻塞，T1 马上执行；等 T1 执行完，再唤醒 T2，也能保证同一时刻只有一个线程执行。但是相比 Hasen 模型，T2 多了一次阻塞唤醒操作。
 MESA 管程里面，T2 通知完 T1 后，T2 还是会接着执行，T1 并不立即执行，仅仅是从条件变量的等待队列进到入口等待队列里面。这样做的好处是 notify() 不用放到代码的最后，
 T2 也没有多余的阻塞唤醒操作。但是也有个副作用，就是当 T1 再次执行的时候，可能曾经满足的条件，现在已经不满足了，所以需要以循环方式检验条件变量。
 
 db.dcs_alliance_contract_test.getIndexes()
 db.runCommand({"dropIndexes":"dcs_alliance_contract_test","index":"contractStartDate_1"}) 
 db.dcs_alliance_contract_test.ensureIndex({"contractStartDate":1})
 db.dcs_alliance_contract_test.find({"contractStartDate":{$gt:new Date("2020-10-22T00:00:00Z")}}).explain()
 
 mysql日志：general_log, redo log, bin log, 中继日志, 慢查询日志
 show variables like '%log';
 back_log, general_log, innodb_api_enable_bin_log, innodb_locks_unsafe_for_bin_log,  log_statements_unsafe_for_binlog, log_sys_log, slow_query_log, sync_binlog, sync_relay_log
 每一个表是好几个B+树
 没有主键的表，innodb会给默认创建一个Rowid做主键
 在 MySQL 5.6 之前，只能从 ID3 开始一个个回表。到主键索引上找出数据行，再对比字段值。而 MySQL 5.6 引入的索引下推优化（index condition pushdown)， 
 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。
 
 1.select * from T where k in(1,2,3,4,5) 
2.select * from T where k between 1 and 5
作者回复: 好问题，
第一个要树搜素5次
第二个搜索一次

每个缓存行64字节

多路复用 一个线程处理多个IO流 允许内核中，同时存在多个监听套接字和已连接套接字 内核会监听连接请求或者数据请求，一旦到达，交给redis处理
达到了一个Redis线程处理多个IO流的效果
epoll
事件处理队列

为了方便你理解，我再以连接请求和读数据请求为例，具体解释一下。这两个请求分别对应 Accept 事件和 Read 事件，Redis 分别对这两个事件注册 accept 和 get 回调函数。
当 Linux 内核监听到有连接请求或读数据请求时，就会触发 Accept 事件和 Read 事件，此时，内核就会回调 Redis 相应的 accept 和 get 函数进行处理。
为事件注册accept和get函数

Redis 单线程是指它对网络 IO 和数据读写的操作采用了一个线程，而采用单线程的一个核心原因是避免多线程开发的并发控制问题。单线程的 Redis 也能获得高性能，
跟多路复用的 IO 模型密切相关，因为这避免了 accept() 和 send()/recv() 潜在的网络 IO 操作阻塞点。

AOF 还有一个好处：它是在命令执行成功后才记录日志，所以不会阻塞当前的写操作。 避免出现记录错误日志

AOF坏处：如果刚执行完一个命令，还没有来得及记日志就宕机了，那么这个命令和相应的数据就有丢失的风险，用作缓存还好，如果用作数据库就不好说了
AOF 虽然避免了对当前命令的阻塞，但可能会给下一个操作带来阻塞风险。这是因为，AOF 日志也是在主线程中执行的，如果在把日志文件写入磁盘时，磁盘写压力大，就会导致写盘很慢，进而导致后续的操作也无法执行了
都和AOF写回磁盘有关
三种写回策略

AOF重写弄新的文件，写入当前数据库的状态值

AOF重写采用了新的进程，总结为，一个拷贝，两处日志。新的操作会被写到正在使用的AOF日志的日志缓冲区和重写的AOF日志缓冲区中  AOF缓冲，AOF重写缓冲区
AOF重写这个过程不会阻塞主线程。

性能和可靠性之间要做一个trade-off

采用AOFredis如果数据量比较大，可能会恢复的比较慢，它记录的是操作命令。有没有可靠的而且在宕机时实现快速恢复的方法呢？RDB 内存快照

Redis 提供了两个命令来生成 RDB 文件，分别是 save 和 bgsave。save：在主线程中执行，会导致阻塞；bgsave：创建一个子进程，专门用于写入 RDB 文件，避免了主线程的阻塞，这也是 Redis RDB 文件生成的默认配置。
在对内存数据做快照时，这些数据还能“动”吗? 

为了快照而暂停写操作，肯定是不能接受的。所以这个时候，Redis 就会借助操作系统提供的写时复制技术（Copy-On-Write, COW），在执行快照的同时，正常处理写操作。
简单来说，bgsave 子进程是由主线程 fork 生成的，可以共享主线程的所有内存数据。bgsave 子进程运行后，开始读取主线程的内存数据，并把它们写入 RDB 文件。

快照的频率不好把握

Redis 4.0 中提出了一个混合使用 AOF 日志和内存快照的方法。简单来说，内存快照以一定的频率执行，在两次快照之间，使用 AOF 日志记录这期间的所有命令操作。

数据不能丢失时，内存快照和 AOF 的混合使用是一个很好的选择；
如果允许分钟级别的数据丢失，可以只使用 RDB；
如果只用 AOF，优先使用 everysec 的配置选项，因为它在可靠性和性能之间取了一个平衡。

死锁代码

public class DeadLockSample extends Thread {
  private String first;
  private String second;
  public DeadLockSample(String name, String first, String second) {
      super(name);
      this.first = first;
      this.second = second;
  }

  public  void run() {
      synchronized (first) {
          System.out.println(this.getName() + " obtained: " + first);
          try {
              Thread.sleep(1000L);
              synchronized (second) {
                  System.out.println(this.getName() + " obtained: " + second);
              }
          } catch (InterruptedException e) {
              // Do nothing
          }
      }
  }
  public static void main(String[] args) throws InterruptedException {
      String lockA = "lockA";
      String lockB = "lockB";
      DeadLockSample t1 = new DeadLockSample("Thread1", lockA, lockB);
      DeadLockSample t2 = new DeadLockSample("Thread2", lockB, lockA);
      t1.start();
      t2.start();
      t1.join();
      t2.join();
  }
}

GC Garbage Collector
Oracle JDK
1、Serial GC，它是最古老的垃圾收集器，“Serial”体现在其收集工作是单线程的，并且在进行垃圾收集过程中，
会进入臭名昭著的“Stop-The-World”状态。
当然，其单线程设计也意味着精简的 GC 实现，无需维护复杂的数据结构，
初始化也简单，所以一直是 Client 模式下 JVM 的默认选项。
从年代的角度，通常将其老年代实现单独称作 Serial Old，它采用了标记 - 整理（Mark-Compact）算法，
区别于新生代的复制算法。Serial GC 的对应 JVM 参数是：
-XX:+UseSerialGC
不直接对可回收对象进行清理，而是让所有可用的对象都向一端移动。然后直接清理掉边界以外的内存。
2、ParNew GC，很明显是个新生代 GC 实现，
它实际是 Serial GC 的多线程版本，最常见的应用场景是配合老年代的 CMS GC 工作，
下面是对应参数-XX:+UseConcMarkSweepGC -XX:+UseParNewGC
3、CMS（Concurrent Mark Sweep） GC，基于标记 - 清除（Mark-Sweep）算法，
设计目标是尽量减少停顿时间，这一点对于 Web 等反应时间敏感的应用非常重要，
一直到今天，仍然有很多系统使用 CMS GC。但是，CMS 采用的标记 - 清除算法，
存在着内存碎片化问题，所以难以避免在长时间运行等情况下发生 full GC，导致恶劣的停顿。
另外，既然强调了并发（Concurrent），CMS 会占用更多 CPU 资源，并和用户线程争抢。
4、Parallel GC，在早期 JDK 8 等版本中，它是 server 模式 JVM 的默认 GC 选择，也被称作是吞吐量优先的 GC。
它的算法和 Serial GC 比较相似，尽管实现要复杂的多，
其特点是新生代和老年代 GC 都是并行进行的，在常见的服务器环境中更加高效。
开启选项是：-XX:+UseParallelGC
5、G1 GC 这是一种兼顾吞吐量和停顿时间的 GC 实现，是 Oracle JDK 9 以后的默认 GC 选项。
G1 可以直观的设定停顿时间的目标，
相比于 CMS GC，G1 未必能做到 CMS 在最好情况下的延时停顿，但是最差情况要好很多。
G1 GC 仍然存在着年代的概念，但是其内存结构并不是简单的条带式划分，而是类似棋盘的一个个 region。
Region 之间是复制算法，但整体上实际可看作是标记 - 整理（Mark-Compact）算法，
可以有效地避免内存碎片，尤其是当 Java 堆非常大的时候，G1 的优势更加明显。
G1 吞吐量和停顿表现都非常不错，并且仍然在不断地完善，
与此同时 CMS 已经在 JDK 9 中被标记为废弃（deprecated），所以 G1 GC 值得你深入掌握。
内存特别大的时候特别适合用G1 基础配置，如堆大小，比较大，比如16g以上，建议优先g1

调用出现异常有可能是出现了jar包冲突的问题




java -XX:+PrintFlagsFinal
可以看到1.8默认的是 UseParallelGC
ParallelGC 默认的是 Parallel Scavenge（新生代）+ Parallel Old（老年代）
在JVM中是+XX配置实现的搭配组合：
UseSerialGC 表示 “Serial” + "Serial Old"组合
UseParNewGC 表示 “ParNew” + “Serial Old”
UseConcMarkSweepGC 表示 “ParNew” + “CMS”. 组合，“CMS” 是针对旧生代使用最多的
UseParallelGC 表示 “Parallel Scavenge” + "Parallel Old"组合
UseParallelOldGC 表示 “Parallel Scavenge” + "Parallel Old"组合
在实践中使用UseConcMarkSweepGC 表示 “ParNew” + “CMS” 的组合是经常使用的
client vm mode（win 32）一直是Seiral GC，servermode下，9以后改为了G1，以前是Parrallel Gc

既不能保证一些多线程程序的正确性，例如最著名的就是双检锁（Double-Checked Locking，DCL）的失效问题，具体可以参考我在第 14 讲对单例模式的说明，
双检锁可能导致未完整初始化的对象被访问，理论上这叫并发编程中的安全发布（Safe Publication）失败。

计算机系统中，默认有两种缓存：CPU 里面的末级缓存，即 LLC，用来缓存内存中的数据，避免每次从内存中存取数据；内存中的高速页缓存，即 page cache，用来缓存磁盘中的数据，避免每次从磁盘中存取数据。

LLC 的大小是 MB 级别，page cache 的大小是 GB 级别，而磁盘的大小是 TB 级别。
这其实包含了缓存的第二个特征：缓存系统的容量大小总是小于后端慢速系统的，我们不可能把所有数据都放在缓存系统中。

旁路缓存
Redis 是一个独立的系统软件，和业务应用程序是两个软件，当我们部署了 Redis 实例后，它只会被动地等待客户端发送请求，然后再进行处理。
所以，如果应用程序想要使用 Redis 缓存，我们就要在程序中增加相应的缓存操作代码。所以，我们也把 Redis 称为旁路缓存，也就是说，读取缓存、读取数据库和更新缓存的操作都需要在应用程序中来完成。

不进行数据淘汰的策略，只有 noeviction 这一种。会进行淘汰的 7 种其他策略。
volatile 过期

在设置了过期时间的数据中进行淘汰，包括 volatile-random、volatile-ttl、volatile-lru、volatile-lfu（Redis  4.0 后新增）四种。
在所有数据范围内进行淘汰，包括 allkeys-lru、allkeys-random、allkeys-lfu（Redis 4.0 后新增）三种。

默认情况下，Redis 在使用的内存空间超过 maxmemory 值时，并不会淘汰数据，也就是设定的 noeviction 策略。

LRU: LRU MRU  删除从LRU删除  访问从MRU进

cluster slots

EVAL lua脚本

三个操作写入一个 Lua 脚本，如下所示：
local currentlycurrent = redis.call("incr", KEYS[1])
if tonumber(current) == 1 then
	redis.call("expire", KEYS[1], 60)
end
假设我们编写的脚本名称为 lua.script，我们接着就可以使用 Redis 客户端，带上 eval 选项，来执行该脚本。脚本所需的参数将通过以下命令中的 keys 和 args 进行传递。
redis-cli  --eval lua.script  keys , args

库存查验，扣减，订单处理
那为啥库存扣减操作不能在数据库执行呢？这是因为，一旦请求查到有库存，
就意味着发送该请求的用户获得了商品的购买资格，用户就会下单了。同时，商品的库存余量也需要减少一个。
如果我们把库存扣减的操作放到数据库执行，会带来两个问题。

订单处理可以在数据库中执行，但库存扣减操作，不能交给后端数据库处理。

为了支撑大量高并发的库存查验请求，我们需要在这个环节使用 Redis 保存库存量，
这样一来，请求可以直接从 Redis 中读取库存并进行查验。

在数据库中处理订单的原因比较简单，我先说下。订单处理会涉及支付、商品出库、物流等多个关联操作，
这些操作本身涉及数据库中的多张数据表，要保证处理的事务性，需要在数据库中完成。
而且，订单处理时的请求压力已经不大了，数据库可以支撑这些订单处理请求。
那为啥库存扣减操作不能在数据库执行呢？这是因为，一旦请求查到有库存，就意味着发送该请求的用户
获得了商品的购买资格，用户就会下单了。同时，商品的库存余量也需要减少一个。
如果我们把库存扣减的操作放到数据库执行，会带来两个问题。额外的开销。
Redis 中保存了库存量，而库存量的最新值又是数据库在维护，所以数据库更新后，还需要和 Redis 
进行同步，这个过程增加了额外的操作逻辑，也带来了额外的开销。下单量超过实际库存量，出现超售。
由于数据库的处理速度较慢，不能及时更新库存余量，这就会导致大量库存查验的请求读取到旧的库存值，
并进行下单。此时，就会出现下单数量大于实际的库存量，导致出现超售，这就不符合业务层的要求了。
所以，我们就需要直接在 Redis 中进行库存扣减。具体的操作是，
当库存查验完成后，一旦库存有余量，我们就立即在 Redis 
中扣减库存。而且，为了避免请求查询到旧的库存值，库存查验和库存扣减这两个操作需要保证原子性。

我给你一个小建议，我们可以使用切片集群中的不同实例来分别保存分布式锁和商品库存信息。
使用这种保存方式后，秒杀请求会首先访问保存分布式锁的实例。如果客户端没有拿到锁，
这些客户端就不会查询商品库存，这就可以减轻保存库存信息的实例的压力了。

使用多个实例的切片集群来分担秒杀请求，是否是一个好方法？

使用切片集群分担秒杀请求，可以降低每个实例的请求压力，前提是秒杀请求可以平均打到每个实例上，否则会出现秒杀请求倾斜的情况，反而会增加某个实例的压力，而且会导致商品没有全部卖出的情况。

但用切片集群分别存储库存信息，缺点是如果需要向用户展示剩余库存，要分别查询多个切片，最后聚合结果后返回给客户端。这种情况下，建议不展示剩余库存信息，直接针对秒杀请求返回是否秒杀成功即可。

秒杀系统最重要的是，把大部分请求拦截在最前面，只让很少请求能够真正进入到后端系统，降低后端服务的压力，
常见的方案包括：页面静态化（推送到CDN）、网关恶意请求拦截、请求分段放行、缓存校验和扣减库存、消息队列处理订单。

另外，为了不影响其他业务系统，秒杀系统最好和业务系统隔离，主要包括应用隔离、部署隔离、数据存储隔离。

redis集群的数据倾斜

如果的确有大量的慢查询命令，有两种处理方式：用其他高效命令代替。
比如说，如果你需要返回一个 SET 中的所有成员时，不要使用 SMEMBERS 命令，
而是要使用 SSCAN 多次迭代返回，避免一次返回大量数据，造成线程阻塞。当你需要执行排序、交集、并集操作时，
可以在客户端完成，而不要用 SORT、SUNION、SINTER 这些命令，以免拖慢 Redis 实例。

过期时间加上一个随机数，避免大量key同时过期。

slowlog get
sadd srem

查看redis的进程号
redis-cli info | grep process_id
cd /proc/5332
cat smaps | egrep '^(Swap|Size)'
https://time.geekbang.org/column/article/287819

如果采用了内存大页，那么，即使客户端请求只修改 100B 的数据，Redis 也需要拷贝 2MB 的大页。
相反，如果是常规内存页机制，只用拷贝 4KB。两者相比，你可以看到，当客户端请求修改或新写入数据较多时，内存大页机制将导致大量的拷贝，
这就会影响 Redis 正常的访存操作，最终导致性能变慢。那该怎么办呢？很简单，关闭内存大页，就行了。首先，我们要先排查下内存大页。
方法是：在 Redis 实例运行的机器上执行如下命令:
cat /sys/kernel/mm/transparent_hugepage/enabled
如果执行结果是 always，就表明内存大页机制被启动了；如果是 never，就表示，内存大页机制被禁止。

在实际生产环境中部署时，我建议你不要使用内存大页机制，操作也很简单，只需要执行下面的命令就可以了：
echo never /sys/kernel/mm/transparent_hugepage/enabled

 cluster-node-timeout
 配置项 cluster-node-timeout 定义了集群实例被判断为故障的心跳超时时间，默认是 15 秒。如果 cluster-node-timeout 值比较小，那么，
 在大规模集群中，就会比较频繁地出现 PONG 消息接收超时的情况，从而导致实例每秒要执行 10 次“给 PONG 消息超时的实例发送 PING 消息”这个操作。
 所以，为了避免过多的心跳消息挤占集群带宽，我们可以调大 cluster-node-timeout 值，比如说调大到 20 秒或 25 秒。这样一来， 
 PONG 消息接收超时的情况就会有所缓解，单实例也不用频繁地每秒执行 10 次心跳发送操作了。当然，我们也不要把 cluster-node-timeout 调得太大，
 否则，如果实例真的发生了故障，我们就需要等待 cluster-node-timeout 时长后，才能检测出这个故障，这又会导致实际的故障恢复时间被延长，会影响到集群服务的正常使用。
 
 最后，我也给你一个小建议，虽然我们可以通过调整 cluster-node-timeout 配置项减少心跳消息的占用带宽情况，但是，在实际应用中，如果不是特别需要大容量集群，
 我建议你把 Redis Cluster 的规模控制在 400~500 个实例。假设单个实例每秒能支撑 8 万请求操作（8 万 QPS），每个主实例配置 1 个从实例，
 那么，400~ 500 个实例可支持 1600 万~2000 万 QPS（200/250 个主实例 *8 万 QPS=1600/2000 万 QPS），这个吞吐量性能可以满足不少业务应用的需求。
 
 hash：压缩类型 哈希表
 
 列表查询优化
 
 list hash zset：压缩列表
 
 整数是常用的数据类型，Redis 内部维护了 0 到 9999 这 1 万个整数对象，并把这些整数作为一个共享池使用。
 
 Redis 单实例的内存大小都不要太大，根据我自己的经验值，建议你设置在 2~6GB 。这样一来，无论是 RDB 快照，还是主从集群进行数据同步，都能很快完成，不会阻塞正常请求的处理。
 \
 {$set:{"stateId":7,"stateName":"Deleted"}}
 {"contractNum":{$in:["V20120200822J0010004",
"V20120190822J0010001",
"V20120190822J0010002",
"V20120200822J0010003",
"V20120200822J0010002",
"V20120200822J0010001",
"V20120190824J0010002",
"V20120200824J0010003",
"V20120200824J0010004",
"V20120200825J0010002",
"V20120200825J0010001",
"V56120190824J0010001"]}}

当主从库断连后，主库会把断连期间收到的写操作命令，写入 replication buffer，同时也会把这些操作命令也写入 repl_backlog_buffer 这个缓冲区。
repl_backlog_buffer 是一个环形缓冲区，主库会记录自己写到的位置，从库则会记录自己已经读到的位置。

 RocketMQ 为每个消费组在每个队列上维护一个消费位置（Consumer Offset）
 
 每个消费组都有一个队列？组内竞争，通过消费位置
 答：不是的。消费者组和队列数没有关系。队列的数量根据数据量和消费速度来配置
 
 常用的消息队列中，RabbitMQ 采用的是队列模型，但是它一样可以实现发布 - 订阅的功能。RocketMQ 和 Kafka 采用的是发布 - 订阅模型，并且二者的消息模型是基本一致的。
 
 刚刚我在介绍 RocketMQ 的消息模型时讲过，在消费的时候，为了保证消息的不丢失和严格顺序，每个队列只能串行消费，
 无法做到并发，否则会出现消费空洞的问题。那如果放宽一下限制，不要求严格顺序，能否做到单个队列的并行消费呢？
 如果可以，该如何实现？
 到位的总结：
 RocketMQ业务模型的理解，老师有空帮忙看下哦
1、主题（topic）中有多个队列（队列数量可以水平进行扩容），生产者将其消息发送给主题中的某个队列（根据一定的路由规则，比如取模之类的），
主题不保证消息的有序，只有队列中的消息才是有序的。
2、从主题中的所有队列中取出消息给所有消费组进行消费，消息只能被消费组中的一个线程进行消费，有点类似线程池的形式，工作线程消费来自不同队列的消息，
感觉这也是RocketMq,低时延的原因，不同队列中的消息可以同时被消费，并且消费组的线程也可以并发的消费不同的消息。
3、由于主题中的一个队列都会被多个消费组进行消费，为此需要为每个消费组的消费的不同队列为此一个下标(每个消费组可以一直消费队列中的消息，无需等待其他消费组的确认)，
主题中的队列消息是有序的，为此需要等到所有消费组对此条消息进行确认，才能从队列中移除，感觉每个消费组的队列下标，
可以一个队列维护一个CurrentHashMap来为此每个消费组的下标，这样的话可以防止锁的竞争。
课后习题：尝试回答下课后习题，感觉队列可以维护一个全局的下标，消费队列时，使用CAS进行下标的获取，
由于不保证消息消费的有序，这样的话可以并发的消费消息，由于有全局下标，不会出现获取队列的空洞消息。

我们在设计系统的时候，一定要保证消费端的消费性能要高于生产端的发送性能，这样的系统才能健康的持续运行。

消息队列的优化：
消费端的性能优化除了优化消费业务逻辑以外，也可以通过水平扩容，增加消费端的并发数来提升总体的消费性能。
特别需要注意的一点是，在扩容 Consumer 的实例数量的同时，必须同步扩容主题中的分区（也叫队列）数量，
确保 Consumer 的实例数和分区数量是相等的。如果 Consumer 的实例数量超过分区数量，这样的扩容实际上是没有效果的。
原因我们之前讲过，因为对于消费者来说，在每个分区上实际上只能支持单线程消费。

消费组，主题，生产者

分布式事务
首先，订单系统在消息队列上开启一个事务。然后订单系统给消息服务器发送一个“半消息”，这个半消息不是说消息内容不完整，它包含的内容就是完整的消息内容，
半消息和普通消息的唯一区别是，在事务提交之前，对于消费者来说，这个消息是不可见的。半消息发送成功后，订单系统就可以执行本地事务了
，在订单库中创建一条订单记录，并提交订单库的数据库事务。然后根据本地事务的执行结果决定提交或者回滚事务消息。如果订单创建成功，那就提交事务消息，
购物车系统就可以消费到这条消息继续后续的流程。如果订单创建失败，那就回滚事务消息，购物车系统就不会收到这条消息。这样就基本实现了“要么都成功，要么都失败”的一致性要求。

MQ代码：
你在编写发送消息代码时，需要注意，正确处理返回值或者捕获异常，就可以保证这个阶段的消息不会丢失。
以 Kafka 为例，我们看一下如何可靠地发送消息：同步发送时，只要注意捕获异常即可。
try {    RecordMetadata metadata = producer.send(record).get();    
System.out.println("消息发送成功。");} 
catch (Throwable e) {    System.out.println("消息发送失败！");   
 System.out.println(e);}
 异步发送时，则需要在回调方法里进行检查。
 这个地方是需要特别注意的，很多丢消息的原因就是，我们使用了异步发送，却没有在回调中检查发送结果。
 producer.send(record, (metadata, exception) ->
 {    if (metadata != null) {        System.out.println("消息发送成功。");    } 
 else {        System.out.println("消息发送失败！");
 System.out.println(exception);    }});
 
 警惕消费突然积压：一个慢，一个快
 能导致积压突然增加，最粗粒度的原因，只有两种：要么是发送变快了，要么是消费变慢了。
 
 
 同步使线程在等待
 
 方法多了一个参数用户回调 在Java中，通过传入一个回调类的实例实现类似的功能
 
 异步减少或者避免线程等待，提高吞吐量
 使用异步编程模型，虽然并不能加快程序本身的速度，但可以减少或者避免线程等待，只用很少的线程就可以达到超高的吞吐能力。
 
 关于异步回调机制：
 异步回调机制的本质是通过减少线程等待时占用的CPU时间片，来提供CPU时间片的利用率。
具体做法是用少数线程响应业务请求，但处理时这些线程并不真正调用业务逻辑代码，
而是简单的把业务处理逻辑扔到另一个专门执行业务逻辑代码的线程后就返回了，故不会有任何等待(CPU时间片浪费)。
专门执行业务逻辑的线程可能会由于IO慢导致上下文切换而浪费一些CPU时间片，但这已经不影响业务请求的响应了，
而业务逻辑执行完毕后会把回调处理逻辑再扔到专门执行回调业务逻辑的线程中，这时的执行业务逻辑线程的使命已完成，线程返回，
然后会去找下一个需要执行的业务逻辑，这里也没有任何等待。回调业务处理线程也是同理。
以上于《摩登时代》里的卓别林很像，每个人只做自己的那点事(卓别林只拧螺丝)。有的线程只负责响应请求(放螺丝)，
有的线程只负责执行业务逻辑(拧螺丝)，有的线程只负责执行回调代码(敲螺丝)，
完成后就返回并继续执行下一个相同任务(拧完这个螺丝再找下一个需要拧的螺丝)，
没有相互依赖的等待(放螺丝的不等螺丝拧好就直接放下一个螺丝)。
有利就有弊，分开后是不用等别人了，但想知道之前的步骤是否已经做好了就难了。比如螺丝没有拧紧就开始敲，会导致固定不住。
如果发现螺丝没拧好，敲螺丝的人就要和工头说这块板要返工，螺丝取下，重新放，重新拧，之后才能敲。
个人感觉把关联性强且无需长时间等待的操作(如大量磁盘或网络IO)打包成同步，其他用异步，
这样可以在规避CPU时间片浪费的同时兼顾了一致性，降低了补偿的频率和开销。

异步的本质：
老师可能里面过多提到线程这两个字，所以很多人把异步设计理解成节约线程，其实李玥老师这里想说明的是异步是用来提高cup的利用率，而不是节省线程。
异步编程是通过分工的方式，是为了减少了cpu因线程等待的可能，让CPU一直处于工作状态。换句话说，如果我们能想办法减少CPU空闲时间，我们的计算机就可以支持更多的线程。
其实线程是一个抽象概念，我们从物理层面理解，就是单位时间内把每毫核分配处理不同的任务，从而提高单位时间内CPU的利用率。
作者回复: 👍👍👍
线程就是为了能自动分配CPU时间片而生的。

程序变慢是什么情况：
老师，有一点不太懂，异步转账时，假如专门几个线程（Threads_quest）处理转账请求，其他的线程处理账户增减金额逻辑，虽然大量请求来的时候，
Threads_quest 这几个线程可以接受请求之后扔给其他线程处理增减金额，但是由于请求量过大，不也会导致其他线程处理变慢吗，导致完整的处理也变慢
作者回复: 首先，你要理解，为什么请求多了程序会变慢这个事儿。

计算的资源，比如说CPU、磁盘IO，它的处理能力是恒定的，都不会因为请求量大而“变慢”。

比如说CPU执行一次加法，任何情况下耗时都是差不多的。

我们所看到的请求量大“系统变慢”的现象，一定是因为某一种资源忙，达到了瓶颈。比如说，一个单核CPU，每做一次加法需要0.1秒，那它每秒最多做10次加法。

一下子100个程序同时都来请求CPU做一次加法，这个CPU就需要10秒才能算完。对于这个CPU，它并没变慢，仍然是每秒做10次加法。

但是对于某一个请求CPU的程序来说，它看到的现象是，我让CPU做了一次加法，它八秒才做完，看起来就是“变慢”了。

所以，程序变慢一定是因为某一个资源忙，遇到了瓶颈。同步程序因为线程数量的限制，它的瓶颈往往是线程数量。并不能发挥服务器的全部处理能力。
异步程序不需要那么多线程，所以可以发挥出服务器的全部处理能力，直到把CPU或者磁盘IO打满，所以要快得多。

go 语言的本质就是 用同步的方式，写出异步的代码。

每个用户过来直接启动一个 go routine ，可以同时启动上百万的协程
go processConn()

而go 的内部 只启动少量线程，io 的等待全部做了异步处理。


需要调用get方法结束
老师你好，CompletableFuture这种回调底层还是forkjoin框架，forkjoin对于io这种操作还是会阻塞线程，而且CompletableFuture默认线程数是与cpu核数一样的。
在现在容器化的场景下，Cpu核数都不会很多（一般都是个位数），那么使用CompletableFuture是执行io操作是不是会更早的无响应？因为个位数的线程很快就都被阻塞了。
作者回复: 这里说一下我的理解：

CompletableFuture还不能等完全同于ForkJoin。
可以简单的理解为

CompletableFuture.then() 等于 Fork
CompletableFuture.get() 等于 Join

但不是所有场景下，CompletableFuture都需要用get()结束的。也就是说，有时候是不需要调用阻塞的get()方法的。

另外，虽然CompletableFuture 默认使用 ForkJoinPool，但你完全可以给它提供一个自定义的执行器。

异步解决资源等待问题，网络io，磁盘io

序列化反序列化
很多处理海量数据的场景中，都需要将对象序列化后，把它们暂时从内存转移到磁盘中，等需要用的时候，再把数据从磁盘中读取出来，反序列化成对象来使用，这样不仅可以长期保存不丢失数据，而且可以节省有限的内存空间。

为什么还需要序列化反序列化？
内存里存的东西，不通用， 不同系统， 不同语言的组织可能都是不一样的， 而且还存在很多引用， 指针，并不是直接数据块。
序列化， 反序列化， 其实是约定一种标准吧， 大家都按这个标准去弄， 就能跨平台 ， 跨语言。

\r\n

TCP 连接它是一个全双工的通道，你可以同时进行数据的双向收发，互相是不会受到任何影响的。要提高吞吐量，应用层的协议也必须支持双工通信。

在这个地方，不少同学还是摆脱不了“一问一答，再问再答”这种人类交流的自然方式对思维的影响，写出来的依然是单工通信的程序，单工通信的性能是远远不如双工通信的，
所以，要想做到比较好的网络传输性能，双工通信的方式才是最佳的选择。

消息队列，异步，缓存，锁的正确使用，线程协调，序列化管理，内存管理
使用异步设计的方法；异步网络 IO；专用序列化、反序列化方法；设计良好的传输协议；双工通信。

kafaka 
1、send方法 批量发送
2、IO读写优化，顺序读写提升IO性能
3、缓存实现 利用PageCache

对于磁盘来说，它有一个特性，就是顺序读写的性能要远远好于随机读写。在 SSD（固态硬盘）上，顺序读写的性能要比随机读写快几倍，如果是机械硬盘，这个差距会达到几十倍。
为什么呢？操作系统每次从磁盘读写数据的时候，需要先寻址，也就是先要找到数据在磁盘上的物理位置，然后再进行数据读写。如果是机械硬盘，这个寻址需要比较长的时间，因为它要移动磁头，
这是个机械运动，机械硬盘工作的时候会发出咔咔的声音，就是移动磁头发出的声音。顺序读写相比随机读写省去了大部分的寻址时间，
它只要寻址一次，就可以连续地读写下去，所以说，性能要比随机读写要好很多。Kafka 就是充分利用了磁盘的这个特性。它的存储设计非常简单，对于每个分区，
它把从 Producer 收到的消息，顺序地写入对应的 log 文件中，一个文件写满了，就开启一个新的文件这样顺序写下去。消费的时候，也是从某个全局的位置开始，
也就是某一个 log 文件中的某个位置开始，顺序地把消息读出来。这样一个简单的设计，充分利用了顺序读写这个特性，极大提升了 Kafka 在使用磁盘时的 IO 性能。

PageCache
PageCache 就是操作系统在内存中给磁盘上的文件建立的缓存。无论我们使用什么语言编写的程序，在调用系统的 API 读写文件的时候，
并不会直接去读写磁盘上的文件，应用程序实际操作的都是 PageCache，也就是文件在内存中缓存的副本。
应用程序在写入文件的时候，操作系统会先把数据写入到内存中的 PageCache，然后再一批一批地写到磁盘上。读取文件的时候，也是从 PageCache 中来读取数据，这时候会出现两种可能情况。

UPDATE dcs_alliance_contract
SET store_name = (
 SELECT
  store_name
 FROM
  dcs_alliance_project_to_store
 WHERE
  dcs_alliance_project_to_store.project_num = dcs_alliance_contract.project_num
)
WHERE identify_code='JM02'

客户端 wait_timeout 默认8h连接器会自动断开
建立连接成本高，使用长连接，但是会占用内存，所以定期断开长连接
大于等于5.7版本，执行一个比较大的查询操作后执行mysql_reset_connection重新初始化连接资源，成本小

git reset --hard commit_id
git push -f origin uat

使用查询缓存 8.0开始没了  select SQL_CACHE * from T where ID=10；
优化器选择索引，表的连接顺序
select * from t1 join t2 using(id) where t1.c = t2.d
你会在数据库的慢查询日志中看到一个 rows_examined 的字段，表示这个语句执行过程中扫描了多少行。
这个值就是在执行器每次调用引擎获取数据行的时候累加的。
在有些场景下，执行器调用一次，在引擎内部则扫描了多行，因此引擎扫描行数跟 rows_examined 并不是完全相同的
create table T(ID int primary key, c int);
更新数据时，IO成本高，采用WAL技术，WriteAheadLogging,先写日志，再写磁盘 redo-log,内存，空闲时写道磁盘
redolog一个文件1G，4个文件环形

write pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。checkpoint 是当前要擦除的位置，
也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。write pos 和 checkpoint 之间的是“粉板”上还空着的部分，可以用来记录新的操作。
如果 write pos 追上 checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。
有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为 crash-safe。

redo-log是InnnoDB特有的日志模块 Server层也有自己的日志 binlog，所有引擎都可以使用

redo是物理日志，循环写，数据页做了什么修改； binlog追加写，逻辑日志 增加了1

redo-prepare binlog redo-commit 执行器生成binlog，引擎  两阶段提交  两份日志逻辑一致

备份扩容：全量备份加binlog

innodb_flush_log_at_trx_commit sync_binlog
redo log 用于保证 crash-safe 能力。 innodb_flush_log_at_trx_commit 这个参数设置成 1 的时候，表示每次事务的 redo log 都直接持久化到磁盘。这个参数我建议你设置成 1，这样可以保证 MySQL 异常重启之后数据不丢失。
sync_binlog 这个参数设置成 1 的时候，表示每次事务的 binlog 都持久化到磁盘。这个参数我也建议你设置成 1，这样可以保证 MySQL 异常重启之后 binlog 不丢失。

Redo log不是记录数据页“更新之后的状态”，而是记录这个页 “做了什么改动”。

Binlog有两种模式，statement 格式的话是记sql语句， row格式会记录行的内容，记两条，更新前和更新后都有。

for(int j=0;j<i;j++) {
	dp[i] = Math.max(dp[i], dp[j]+1);
}
mysql 中事务支持是引擎层实现的

多个事务同时执行出现的问题： 脏读 不可重复读 幻读
隔离级别： 读未提交 读已提交 可重复读 串行化
读未提交是指，一个事务还没提交时，它做的变更就能被别的事务看到。
读提交是指，一个事务提交之后，它做的变更才会被其他事务看到。
可重复读是指，一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。事务在执行期间看到的数据前后必须是一致的
串行化，顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。

隔离级别实现：视图
在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。在“可重复读”隔离级别下，这个视图是在事务启动时创建的，
整个事务存在期间都用这个视图。在“读提交”隔离级别下，这个视图是在每个 SQL 语句开始执行的时候创建的。
这里需要注意的是，“读未提交”隔离级别下直接返回记录上的最新值，没有视图概念；而“串行化”隔离级别下直接用加锁的方式来避免并行访问。

Oracle默认级别读提交；Oracle迁移mysql 改为读提交
show variables like 'transaction_isolation'
可重复度场景：校对数据 事务启动时隔离级别是动态的。

可重复读  事务隔离实现：在 MySQL 中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。

回滚日志总不能一直保留吧，什么时候删除呢？答案是，在不需要的时候才删除。也就是说，系统会判断，
当没有事务再需要用到这些回滚日志时，回滚日志会被删除。什么时候才不需要了呢？
就是当系统里没有比这个回滚日志更早的 read-view 的时候。基于上面的说明，我们来讨论一下为什么建议你尽量不要使用长事务。

同一条记录在数据库中存在多个版本，MVCC

在 MySQL 5.5 及以前的版本，回滚日志是跟数据字典一起放在 ibdata 文件里的，即使长事务最终提交，回滚段被清理，文件也不会变小。
我见过数据只有 20GB，而回滚段有 200GB 的库。最终只好为了清理回滚段，重建整个库。
除了对回滚段的影响，长事务还占用锁资源，也可能拖垮整个库，这个我们会在后面讲锁的时候展开。
、
长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，
所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。

begin/start transaction commit rollback
建议set autocommit=1

查找持续时间超过 60s 的事务。
select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))>60

二叉搜索树特点：左小右大

多叉树：索引不止存于内存中，还在磁盘上 减少访问磁盘的次数

以 InnoDB 的一个整数字段索引为例，这个 N 差不多是 1200。这棵树高是 4 的时候，就可以存 1200 的 3 次方个值，
这已经 17 亿了。考虑到树根的数据块总是在内存中的，一个 10 亿行的表上一个整数字段的索引，查找一个值最多只需要访问 3 次磁盘。
其实，树的第二层也有很大概率在内存中，那么访问磁盘的平均次数就更少了。

N 叉树由于在读写上的性能优点，以及适配磁盘的访问模式，已经被广泛应用在数据库引擎中了。

索引是在存储引擎层实现的。表都是根据主键顺序以索引的形式存放的，索引组织表
每个索引对应一个B+树
根据叶子节点的内容，索引分为主键索引和非主键索引
主键索引叶子节点存的是整行数据 非主键索引叶子节点存的是主键的值

数据页分裂与合并：
而更糟的情况是，如果 R5 所在的数据页已经满了，根据 B+ 树的算法，这时候需要申请一个新的数据页，然后挪动部分数据过去。
这个过程称为页分裂。在这种情况下，性能自然会受影响。除了性能外，页分裂操作还影响数据页的利用率。
原本放在一个页的数据，现在分到两个页中，整体空间利用率降低大约 50%。当然有分裂就有合并。
当相邻两个页由于删除了数据，利用率很低之后，会将数据页做合并。合并的过程，可以认为是分裂过程的逆过程。

插入数据时主键索引不挪动数据记录，非主键索引触发叶子节点的分裂

主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小

尽量使用主键索引

避免长事务
	可以去掉读事务
	业务连接数据库的时候，根据业务本身的预估，通过 SET MAX_EXECUTION_TIME 命令，来控制每个语句执行的最长时间，避免单个语句意外执行太长时间。
	确认是否使用了 set autocommit=0。

general_log将所有到达MySQL Server的SQL语句记录下来

监控 information_schema.Innodb_trx 表，设置长事务阈值，超过就报警 / 或者 kill；
Percona 的 pt-kill 这个工具不错，推荐使用；
在业务功能测试阶段要求输出所有的 general_log，分析日志行为提前发现问题；
如果使用的是 MySQL  5.6 或者更新版本，把 innodb_undo_tablespaces 设置成 2（或更大的值）。
如果真的出现大事务导致回滚段过大，这样设置后清理起来更方便。

每一个表是好几个B+树

MySQL5.6中开始支持把undo log分离到独立的表空间，并放到单独的文件目录下。
这给部署不同IO类型的文件位置带来便利，对于并发写入型负载，可以把undo文件部署到单独的高速SSD存储设备上。
innodb_undo_tablespaces[=4]	
用于设定创建的undo表空间的个数，在mysql_install_db时初始化后，就再也不能被改动了，修改该值会导致MySQL无法启动。

默认值为0，表示不独立设置undo的tablespace，默认记录到ibdata中；否则，则在undo目录下创建这么多个undo文件(每个文件的默认大小为10M)。最多可以设置到126。

例如假定设置该值为4，那么就会在mysql的data目录下创建命名为undo001~undo004的undo tablespace文件。
【不支持后期修改】

覆盖索引，减少回表次数；
前缀索引：复用
索引下推： 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。
mysql> select * from tuser where name like '张%' and age=10 and ismale=1;

alter table T engine=InnoDB

数据页

全局锁 表级锁 行锁

如果执行 FTWRL 命令之后由于客户端发生异常断开，那么 MySQL 会自动释放这个全局锁，整个库回到可以正常更新的状态。
而将整个库设置为 readonly 之后，如果客户端发生异常，则数据库就会一直保持 readonly 状态，这样会导致整个库长时间处于不可写状态，风险较高。

表锁的语法是 lock tables … read/write。与 FTWRL 类似，可以用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放。
需要注意，lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。
举个例子, 如果在某个线程 A 中执行 lock tables t1 read, t2 write; 这个语句，则其他线程写 t1、读写 t2 的语句都会被阻塞。
同时，线程 A 在执行 unlock tables 之前，也只能执行读 t1、读写 t2 的操作。连写 t1 都不允许，自然也不能访问其他表。
在还没有出现更细粒度的锁的时候，表锁是最常用的处理并发的方式。而对于 InnoDB 这种支持行锁的引擎，
一般不使用 lock tables 命令来控制并发，毕竟锁住整个表的影响面还是太大。

git checkout -b changeB origin/changeB


另一类表级的锁是 MDL（metadata lock)。MDL 不需要显式使用，在访问一个表的时候会被自动加上。
MDL 的作用是，保证读写的正确性。你可以想象一下，如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，
删了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的。因此，在 MySQL 5.5 版本中引入了 MDL，当对一个表做增删改查操作的时候，
加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁。读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。
读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。

重试机制，加个字段，  DDL阻塞问题  链式效应 防止饿死
比如 启动一个事务查询，接下来alter会阻塞，然后查询也会阻塞
如果某个表上的查询语句频繁，而且客户端有重试机制，也就是说超时后会再起一个新 session 再请求的话，这个库的线程很快就会爆满。
你现在应该知道了，事务中的 MDL 锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放。

两阶段锁？
在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。
这个就是两阶段锁协议。知道了这个设定，对我们使用事务有什么帮助呢？
那就是，如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。我给你举个例子。

死锁和死锁检测
update set id =1 ; update set id=2
出现死锁之后，两种策略，
1、一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout 来设置。默认50s 太短又会误伤
2、另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑。但是耗费CPU资源

所以控制并发度 客户端做并发控制 相同的更新，采用中间件
一行逻辑改成多行，减少锁等待个数

如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁的申请时机尽量往后放。

第一种，直接执行 delete from T limit 10000;第二种，在一个连接中循环执行 20 次 delete from T limit 500;第三种，在 20 个连接中同时执行 delete from T limit 500。
1.直接delete 10000可能使得执行事务时间过长
2.效率慢点每次循环都是新的短事务，并且不会锁同一条记录
3.效率虽高，但容易锁住同一条记录，发生死锁的可能性比较高

但是，我在上一篇文章中，和你分享行锁的时候又提到，一个事务要更新一行，如果刚好有另外一个事务拥有这一行的行锁，
它又不能这么超然了，会被锁住，进入等待状态。问题是，既然进入了等待状态，那么等到这个事务自己获取到行锁要更新数据的时候，它读到的值又是什么呢？

begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作 InnoDB 表的语句，事务才真正启动。
如果你想要马上启动一个事务，可以使用 start transaction with consistent snapshot 这个命令。

一致性视图 支持读提交和可重复读
第一种启动方式，一致性视图是在执行第一个快照读语句时创建的；
第二种启动方式，一致性视图是在执行 start transaction with consistent snapshot 时创建的。

按照可重复读的定义，一个事务启动的时候，能够看到所有已经提交的事务结果。
但是之后，这个事务执行期间，其他事务的更新对它不可见。
因此，一个事务只需要在启动的时候声明说，“以我启动的时刻为准，如果一个数据版本是在我启动之前生成的，就认；
如果是我启动以后才生成的，我就不认，我必须要找到它的上一个版本”。当然，如果“上一个版本”也不可见，那就得继续往前找。
还有，如果是这个事务自己更新的数据，它自己还是要认的。

在实现上， InnoDB 为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务 ID。“活跃”指的就是，启动了但还没提交。也就是下面的未提交事务集合

数据版本可见性规则：数组里面的值
已提交事务 （低水位） 未提交事务集合 （高水位） 未开始事务

对于当前事务的启动瞬间来说，一个数据版本的 row trx_id，有以下几种可能：
如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的；
如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的；
如果落在黄色部分，那就包括两种情况a.  若 row trx_id 在数组中，表示这个版本是由还没提交的事务生成的，不可见；b.  若 row trx_id 不在数组中，表示这个版本是已经提交了的事务生成的，可见。

InnoDB 利用了“所有数据都有多个版本”的这个特性，实现了“秒级创建快照”的能力。

视图数组：
事务 A 开始前，系统里面只有一个活跃事务 ID 是 99；事务 A、B、C 的版本号分别是 100、101、102，且当前系统里只有这四个事务；三个事务开始前，(1,1）这一行数据的 row trx_id 是 90。
这样，事务 A 的视图数组就是[99,100], 事务 B 的视图数组是[99,100,101], 事务 C 的视图数组是[99,100,101,102]。

select当前读：
其实，除了 update 语句外，select 语句如果加锁，也是当前读。所以，如果把事务 A 的查询语句 select * from t where id=1 修改一下，
加上 lock in share mode 或 for update，也都可以读到版本号是 101 的数据，返回的 k 的值是 3。下面这两个 select 语句，
就是分别加了读锁（S 锁，共享锁）和写锁（X 锁，排他锁）。mysql> select k from t where id=1 lock in share mode;mysql> select k from t where id=1 for update;

一致性读、当前读和行锁就串起来了。

在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图

数据的最新版本是一个事务的id
从图中可以看到，第一个有效更新是事务 C，把数据从 (1,1) 改成了 (1,2)。这时候，这个数据的最新版本的 row trx_id 是 102，而 90 这个版本已经成为了历史版本。
第二个有效更新是事务 B，把数据从 (1,2) 改成了 (1,3)。这时候，这个数据的最新版本（即 row trx_id）是 101，而 102 又成为了历史版本。

可重复读共用一个一致性视图
读提交每个语句执行前都会重新算出一个新的视图

表结构不支持可重复读。表结构没有对应的行数据，也没有row trx_id 只能遵循当前读的逻辑

对 InnoDB 表更新一行，可能过了 MDL 关，却被挡在行锁阶段。
45讲8chapter课后答案
假设有两个事务A和B， 且A事务是更新c=0的事务； 给定条件： 
1， 事务A update 语句已经执行成功， 说明没有另外一个活动中的事务在执行修改条件为id in 1,2,3,4或c in 1,2,3,4, 否则update会被锁阻塞； 
2，事务A再次执行查询结果却是一样， 说明什么？说明事务B把id或者c给修改了， 而且已经提交了， 导致事务A“当前读”没有匹配到对应的条件； 
事务A的查询语句说明了事务B执行更新后，提交事务B一定是在事务A第一条查询语句之后执行的； 

所以执行顺序应该是：
1， 事务A select * from t;
2, 事务B update t set c = c + 4; // 只要c或者id大于等于5就行; 当然这行也可以和1调换， 不影响
3, 事务B commit;
4, 事务A update t set c = 0 where id = c; // 当前读； 此时已经没有匹配的行
5， 事务A select * from t;

合同查询线程池
/**
 * 合同查询线程池
 */
@Configuration
public class ContractQueryExecutorConfig {

    @Bean("contractDbQueryExecutorService")
    public ExecutorService getContractQueryExecutorService() {

        ThreadFactory threadFactory = new ThreadFactory() {
            private int count = 0;

            @Override
            public Thread newThread(Runnable r) {
                return new Thread(r, "contract-db-query-" + count++);
            }
        };

        ExecutorService contractQueryThreadPool = new ThreadPoolExecutor(0, 20,
                1L, TimeUnit.HOURS,
                new SynchronousQueue<Runnable>(), threadFactory, new ThreadPoolExecutor.CallerRunsPolicy());
        return contractQueryThreadPool;
    }
}
// 多线程查询
	Map<Future<HttpResponse<MdSupplyPriceResultModel>>,PurchaseOrderRequestModel> taskMap = new ConcurrentHashMap();
	for (PurchaseOrderRequestModel model : models) {
		Future<HttpResponse<MdSupplyPriceResultModel>> task = contractDbQueryExecutorService.submit(()->getNewSupplyPrice(model));
		taskMap.put(task,model);
	}
	
	@Autowired
    private ExecutorService contractDbQueryExecutorService;
	
抽象类定义抽象方法子类必须重写

redo log包含了数据的变更和change buffer的变更
具体过程：
merge那段的解释明白了change buffer操作逻辑。
即change buffer变化与数据块变化是分开的，最初redo中记录的只是change buffer的变更，
因为还未应用到数据块上。而merge后redo记录的是数据块、change buffer的变更。

如果要简单地对比这两个机制在提升更新性能上的收益的话，
redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写），
而 change buffer 主要节省的则是随机读磁盘的 IO 消耗。

set long_query_time=0;
select * from t where a between 10000 and 20000; /*Q1*/
select * from t force index(a) where a between 10000 and 20000;/*Q2*/

我理解 session A 开启的事务对 session B的delete操作后的索引数据的统计时效产生了影响，
因为需要保证事务A的重复读，在数据页没有实际删除，而索引的统计选择了N个数据页，
这部分数据页不收到前台事务的影响，所以整体统计值会变大，直接影响了索引选择的准确性；

删的时候，由于有未提交事务开启的一致性视图read-view，所以导致了存在两个数据版本的数据，
貌似优化器在"看"二级索引的时候，"看到"了多个历史版本的数据，错误以为有很多数据
而主键索引数量由于确认机制不同，数量没有变，综合考虑，优化器选择了主键索引

mysql> select
  count(distinct left(email,4)）as L4,
  count(distinct left(email,5)）as L5,
  count(distinct left(email,6)）as L6,
  count(distinct left(email,7)）as L7,
from SUser;

内存数据页和磁盘数据页
当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。
内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。

首先，你要正确地告诉 InnoDB 所在主机的 IO 能力，这样 InnoDB 才能知道需要全力刷脏页的时候，可以刷多快。
这就要用到 innodb_io_capacity 这个参数了，它会告诉 InnoDB 你的磁盘能力。
这个值我建议你设置成磁盘的 IOPS。磁盘的 IOPS 可以通过 fio 这个工具来测试，下面的语句是我用来测试磁盘随机读写的命令：

多关注脏页比例：
平时要多关注脏页比例，不要让它经常接近 75%。
脏页比例是通过 Innodb_buffer_pool_pages_dirty/Innodb_buffer_pool_pages_total 得到的，具体的命令参考下面的代码：
mysql> select VARIABLE_VALUE into @a from global_status where VARIABLE_NAME = 'Innodb_buffer_pool_pages_dirty';
       select VARIABLE_VALUE into @b from global_status where VARIABLE_NAME = 'Innodb_buffer_pool_pages_total';
       select @a/@b;

一旦一个查询请求需要在执行过程中先 flush 掉一个脏页时，这个查询就可能要比平时慢了。
而 MySQL 中的一个机制，可能让你的查询会更慢：在准备刷一个脏页的时候，如果这个数据页旁边的数据页刚好是脏页，
就会把这个“邻居”也带着一起刷掉；而且这个把“邻居”拖下水的逻辑还可以继续蔓延，
也就是对于每个邻居数据页，如果跟它相邻的数据页也还是脏页的话，也会被放到一起刷。在 InnoDB 中，innodb_flush_neighbors 参数

找“邻居”这个优化在机械硬盘时代是很有意义的，可以减少很多随机 IO。
机械硬盘的随机 IOPS 一般只有几百，相同的逻辑操作减少随机 IO 就意味着系统性能的大幅度提升。
而如果使用的是 SSD 这类 IOPS 比较高的设备的话，我就建议你把 innodb_flush_neighbors 的值设置成 0。因为这时候 IOPS 往往不是瓶颈，
而“只刷自己”，就能更快地执行完必要的刷脏页操作，减少 SQL 语句响应时间。在 MySQL 8.0 中，innodb_flush_neighbors 参数的默认值已经是 0 了。

InnoDB 会在后台刷脏页，而刷脏页的过程是要将内存页写入磁盘。所以，无论是你的查询语句在需要内存的时候可能要求淘汰一个脏页，
还是由于刷脏页的逻辑会占用 IO 资源并可能影响到了你的更新语句，都可能是造成你从业务端感知到 MySQL“抖”了一下的原因。

redo log还记录了change buffer的改变,那么还要把change buffer purge到idb以及merge change buffer.merge生成的数据页也是脏页,也要持久化到磁盘

下面两个相除得到脏页比例
SHOW GLOBAL STATUS LIKE 'Innodb_buffer_pool_pages_dirty'; 0
SHOW GLOBAL STATUS LIKE 'Innodb_buffer_pool_pages_total';  262112

SHOW GLOBAL STATUS LIKE 'Innodb_buffer_pool_pages_data'; 188771

SHOW GLOBAL VARIABLES LIKE 'innodb_buffer_pool_size'; 4294967296

SHOW GLOBAL STATUS LIKE 'Innodb_page_size';  16384


SHOW GLOBAL STATUS LIKE 'Innodb_buffer_pool_pages_data'; 188771
show global variables like '%capacity%'; innodb_io_capacity:4000    innodb_io_capacity_max:8000
 Innodb_buffer_pool_pages_data / Innodb_buffer_pool_pages_total * 100%

1：MySQL抖一下是什么意思？

抖我认为就是不稳定的意思，一个SQL语句平时速度都挺快的，偶尔会慢一下且没啥规律，就是抖啦！

2：MySQL为啥会抖一下？

因为运行的不正常了，或者不稳定了，需要花费更多的资源处理别的事情，会使SQL语句的执行效率明显变慢。
针对innoDB导致MySQL抖的原因，主要是InnoDB 会在后台刷脏页，而刷脏页的过程是要将内存页写入磁盘。
所以，无论是你的查询语句在需要内存的时候可能要求淘汰一个脏页，
还是由于刷脏页的逻辑会占用 IO 资源并可能影响到了你的更新语句，都可能是造成你从业务端感知MySQL“抖”了一下的原因。

3：MySQL抖一下有啥问题？

很明显系统不稳定，性能突然下降对业务端是很不友好的。

4：怎么让MySQL不抖？

设置合理参数配配置，尤其是设置 好innodb_io_capacity 的值，并且平时要多关注脏页比例，不要让它经常接近 75%

5：啥是脏页？

当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。

按照这个定义感觉脏页是不可避免的，写的时候总会先写内存再写磁盘和有没有用WAL没啥关系？

6：啥是干净页？

内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。

7：脏页是咋产生的？

因为使用了WAL技术，这个技术会把数据库的随机写转化为顺序写，但副作用就是会产生脏页。

8：啥是随机写？为啥那么耗性能？

随机写我的理解是，这次写磁盘的那个扇区和上一次没啥关系，需要重新定位位置，机械运动是很慢的即使不是机械运动重新定位写磁盘的位置也是很耗时的。

9：啥是顺序写？

顺序写我的理解是，这次写磁盘那个扇区就在上一次的下一个位置，不需要重新定位写磁盘的位置速度当然会快一些。

10：WAL怎么把随机写转化为顺序写的？

写redolog是顺序写的，先写redolog等合适的时候再写磁盘，间接的将随机写变成了顺序写，性能确实会提高不少。

100M的redo很容易写满，系统锁死，触发检查点推进，导致写操作卡住。由于主机IO能力很强，检查点会很快完成，卡住的写操作又很快可以执行。循环往复，现象就是写操作每隔一小段时间执行就会变慢几秒。

把该讲内容总结为几个问题, 大家复习的时候可以先尝试回答这些问题检查自己的掌握程度:

1.
 脏页和干净页的定义是什么?
2.
引发数据库flush脏页的四种典型场景各是什么?对mysql性能的影响各是怎样的? 把内存数据写进磁盘
InnoDB的redo log写满了
系统内存不足。当需要新的内存页，而内存不够用的时候，就要淘汰一些数据页，空出内存给别的数据页使用。如果淘汰的是“脏页”，就要先将脏页写到磁盘。
    你一定会说，这时候难道不能直接把内存淘汰掉，下次需要请求的时候，从磁盘读入数据页，然后拿redo log出来应用不就行了这里其实是从性能考虑的
    第二种是“内存不够用了，要先将脏页写到磁盘”，这种情况其实是常态。InnoDB用缓冲池(buffer pool）管理内存，缓冲池中的内存页有三种状态:

    第一种是，还没有使用的;
    第二种是，使用了并且是干净页;
    第三种是，使用了并且是脏页。
MSQL认为系统"空闲”的时候
MSQL正常关闭的情况
3.
缓存池中的内存页有哪三种状态? 哪两种刷脏页的情况会比较影响性能?
4.
innodb_io_capacity这个参数的作用是什么,这个参数设置错误可能导致什么样的后果, 如何正确的设置这个参数?
5.
mysql如何通过innodb_io_capacity, 脏页比例(M), 当前日志序号(N)这三个指标来控制以什么样的速度去刷脏页的?
6.
innodb_flush_neighbor这个参数表示什么意思,应该如何设置?

desc SELECT * FROM md_branch;

如果使用undo log作为持久化数据，意味着修改数据和undo log必须同时写入磁盘持久化，这必定带来巨大的磁盘io，解决方案为了平衡磁盘io和一致性，引入redo log。
数据和undo log可定时从缓冲刷至磁盘，但是redo log必须实时写入磁盘，当系统奔溃时，可依据redo log进行数据重做。

relay（中继） log
Mysql 主节点将binlog写入本地，从节点定时请求增量binlog，主节点将binlog同步到从节点。
从节点单独进程会将binlog 拷贝至本地 relaylog中。
从节点定时重放relay log。

redo日志设置太小，内存又比较大，导致innodb缓存的脏页还没多少就开始大量flush，刷写频率增大。
感觉有点像jvm中，年轻代内存设置小点，导致频繁younggc。当然这就是个权衡，毕竟redo和内存不能无限大。

一个 InnoDB 表包含两部分，即：表结构定义和数据。
在 MySQL 8.0 版本以前，表结构是存在以.frm 为后缀的文件里。而 MySQL 8.0 版本，则已经允许把表结构定义放在系统数据表中了。
因为表结构定义占用的空间很小，所以我们今天主要讨论的是表数据。

表数据既可以存在共享表空间里，也可以是单独的文件。
这个行为是由参数 innodb_file_per_table 控制的：
这个参数设置为 OFF 表示的是，表的数据放在系统共享表空间，也就是跟数据字典放在一起；
这个参数设置为 ON 表示的是，每个 InnoDB 表数据存储在一个以 .ibd 为后缀的文件中。
MySQL 5.6.6 版本开始，它的默认值就是 ON 了。

我们在删除整个表的时候，可以使用 drop table 命令回收表空间。
但是，我们遇到的更多的删除数据的场景是删除某些行，这时就遇到了我们文章开头的问题：表中的数据被删除了，但是表空间却没有被回收。

delete 命令其实只是把记录的位置，或者数据页标记为了“可复用”，但磁盘文件的大小是不会变的。也就是说，
通过 delete 命令是不能回收表空间的。这些可以复用，而没有被使用的空间，看起来就像是“空洞”。

alter table A engine=InnoDB

analyze table t 其实不是重建表，只是对表的索引信息做重新统计，没有修改数据，这个过程中加了 MDL 读锁；

redo log太小的话：
系统不得不停止所有更新，去推进 checkpoint。这时，你看到的现象就是磁盘压力很小，但是数据库出现间歇性的性能下跌。

id没必要连续，但一定要递增

本来就很紧凑，没能整出多少剩余空间。
重新收缩的过程中，页会按90%满的比例来重新整理页数据（10%留给UPDATE使用），
未整理之前页已经占用90%以上，收缩之后，文件就反而变大了。

每一行记录都要判断自己是否对这个会话可见
可重复读是它默认的隔离级别，在代码上就是通过多版本并发控制，也就是 MVCC 来实现的。
每一行记录都要判断自己是否对这个会话可见，因此对于 count(*) 请求来说，InnoDB 只好把数据一行一行地读出依次判断，
可见的行才能够用于计算“基于这个查询”的表的总行数。

InnoDB 是索引组织表，主键索引树的叶子节点是数据，而普通索引树的叶子节点是主键值。
所以，普通索引树比主键索引树小很多。对于 count(*) 这样的操作，遍历哪个索引树得到的结果逻辑上都是一样的。
因此，MySQL 优化器会找到最小的那棵树来遍历。在保证逻辑正确的前提下，尽量减少扫描的数据量，是数据库系统设计的通用法则之一。

show table status 的table_rows是通过采样得来的
索引统计的值是通过采样来估算的。

count(*)、count(主键 id) 和 count(1) 都表示返回满足条件的结果集的总行数；
而 count(字段），则表示返回满足条件的数据行里面，参数“字段”不为 NULL 的总个数。

对于 count(主键 id) 来说，InnoDB 引擎会遍历整张表，把每一行的 id 值都取出来，返回给 server 层。server 层拿到 id 后，判断是不可能为空的，就按行累加。
对于 count(1) 来说，InnoDB 引擎遍历整张表，但不取值。server 层对于返回的每一行，放一个数字“1”进去，判断是不可能为空的，按行累加。

但是 count(*) 是例外，并不会把全部字段取出来，而是专门做了优化，不取值。count(*) 肯定不是 null，按行累加。

如果这个“字段”是定义为 not null 的话，一行行地从记录里面读出这个字段，判断不能为 null，按行累加；
如果这个“字段”定义允许为 null，那么执行的时候，判断到有可能是 null，还要把值取出来再判断一下，不是 null 才累加。

count(字段)<count(主键 id)<count(1)≈count(*)

mysql是能确认binlog是完整的
一个事务的 binlog 是有完整格式的：statement 格式的 binlog，最后会有 COMMIT；row 格式的 binlog，最后会有一个 XID event。

curl:
curl -X POST --header 'Content-Type: application/json' --header 'Accept: application/json'
 -d '{"currentPage":1,"pageSize":10,"searchInfo":""}' 'http://localhost:9100/alliance/change/b/main/contract/list'

curl -X GET --header 'Accept: application/json' 'http://10.115.106.18:9100/leasing/dependency/11'

处于 prepare 阶段的 redo log 加上完整 binlog，重启就能恢复，MySQL 为什么要这么设计?
其实，这个问题还是跟我们在反证法中说到的数据与备份的一致性有关。在时刻 B，也就是 binlog 写完以后 MySQL 发生崩溃，
这时候 binlog 已经写入了，之后就会被从库（或者用这个 binlog 恢复出来的库）使用。
所以，在主库上也要提交这个事务。采用这个策略，主库和备库的数据就保证了一致性。

两阶段提交涉及到事务的持久性问题
binlog 还是不能支持崩溃恢复的。我说一个不支持的点吧：binlog 没有能力恢复“数据页”。但是redo log能恢复数据页

InnoDB 引擎使用的是 WAL 技术，执行事务的时候，写完内存和日志，事务就算完成了。如果之后崩溃，要依赖于日志来恢复数据页。
也就是说在图中这个位置发生崩溃的话，事务 1 也是可能丢失了的，而且是数据页级的丢失。
此时，binlog 里面并没有记录数据页的更新细节，是补不回来的。你如果要说，那我优化一下 binlog 的内容，让它来记录数据页的更改可以吗？但，这其实就是又做了一个 redo log 出来。

mysql高可用的基础就是binlog复制，论生态的重要性

redo log太小，WAL机制的能力就发挥不出来了

追问 8：正常运行中的实例，数据写入后的最终落盘，是从 redo log 更新过来的还是从 buffer pool 更新过来的呢？
回答：这个问题其实问得非常好。这里涉及到了，“redo log 里面到底是什么”的问题。实际上，redo log 并没有记录数据页的完整数据，所以它并没有能力自己去更新磁盘数据页，
也就不存在“数据最终落盘，是由 redo log 更新过去”的情况。如果是正常运行的实例的话，数据页被修改以后，跟磁盘的数据页不一致，称为脏页。
最终数据落盘，就是把内存中的数据页写盘。这个过程，甚至与 redo log 毫无关系。
在崩溃恢复场景中，InnoDB 如果判断到一个数据页可能在崩溃恢复的时候丢失了更新，就会将它读到内存，然后让 redo log 更新内存内容。更新完成后，内存页变成脏页，就回到了第一种情况的状态。

redo log buffer 就是一块内存，用来先存 redo 日志的。
也就是说，在执行第一个 insert 的时候，数据的内存被修改了，redo log buffer 也写入了日志。
但是，真正把日志写到 redo log 文件（文件名是 ib_logfile+ 数字），是在执行 commit 语句的时候做的。

uk_user_id_liker_id

每个线程分配一个sort_buffer
Extra 这个字段中的“Using filesort”表示的就是需要排序，MySQL 会给每个线程分配一块内存用于排序，称为 sort_buffer。
sort_buffer_size
“按 name 排序”这个动作，可能在内存中完成，也可能需要使用外部排序，这取决于排序所需的内存和参数 sort_buffer_size。
sort_buffer_size，就是 MySQL 为排序开辟的内存（sort_buffer）的大小。
如果要排序的数据量小于 sort_buffer_size，排序就在内存中完成。但如果排序数据量太大，内存放不下，则不得不利用磁盘临时文件辅助排序。

全字段排序 到主键 id 索引取出整行，取 name、city、age 三个字段的值，存入 sort_buffer 中；
rowid排序 新的算法放入 sort_buffer 的字段，只有要排序的列（即 name 字段）和主键 id。
因为这时候参与排序的行数虽然仍然是 4000 行，但是每一行都变小了，因此需要排序的总数据量就变小了，需要的临时文件也相应地变少了。

如果 MySQL 实在是担心排序内存太小，会影响排序效率，才会采用 rowid 排序算法，这样排序过程中一次可以排序更多行，但是需要再回到原表去取数据。
如果 MySQL 认为内存足够大，会优先选择全字段排序，把需要的字段都放到 sort_buffer 中，这样排序后就会直接从内存里面返回查询结果了，不用再回到原表去取数据。
这也就体现了 MySQL 的一个设计思想：如果内存够，就要多利用内存，尽量减少磁盘访问。

.不会有排序，这种情况属于《高性能mysql》里提到的“in技法”，符合索引的最左原则，是2个等值查询，可以用到右边的索引列。
2.分页查询，可以用延迟关联来优化：
select * from t join
(select id from t where city in('杭州','苏州') order by name limit 10000,100) t_id
on t.id=t_id.id;
事实上会有排序的
虽然有 (city,name) 联合索引，对于单个 city 内部，name 是递增的。
但是由于这条 SQL 语句不是要单独地查一个 city 的值，而是同时查了"杭州"和" 苏州 "两个城市，因此所有满足条件的 name 就不是递增的了

join和left join的区别很大

Extra 字段显示 Using temporary，表示的是需要使用临时表；Using filesort，表示的是需要执行排序操作。

如果你创建的表没有主键，或者把一个表的主键删掉了，那么 InnoDB 会自己生成一个长度为 6 字节的 rowid 来作为主键。

这也就是排序模式里面，rowid 名字的来历。实际上它表示的是：每个引擎用来唯一标识数据行的信息。
对于有主键的 InnoDB 表来说，这个 rowid 就是主键 ID；
对于没有主键的 InnoDB 表来说，这个 rowid 就是由系统生成的；
MEMORY 引擎不是索引组织表。在这个例子里面，你可以认为它就是一个数组。因此，这个 rowid 其实就是数组的下标。

对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。
放弃了树搜索功能，优化器可以选择遍历主键索引，也可以选择遍历索引 t_modified，优化器对比索引大小后发现，索引 t_modified 更小，遍历这个索引比遍历主键索引来得更快。
因此最终还是会选择索引 t_modified。

全索引扫描：
由于在 t_modified 字段加了 month() 函数操作，导致了全索引扫描。
为了能够用上索引的快速定位能力，我们就要把 SQL 语句改成基于字段本身的范围查询。按照下面这个写法，优化器就能按照我们预期的，用上 t_modified 索引的快速定位能力了。
mysql> select count(*) from tradelog where
    -> (t_modified >= '2016-7-1' and t_modified<'2016-8-1') or
    -> (t_modified >= '2017-7-1' and t_modified<'2017-8-1') or
    -> (t_modified >= '2018-7-1' and t_modified<'2018-8-1');

坑：
对于 select * from tradelog where id + 1 = 10000 这个 SQL 语句，这个加 1 操作并不会改变有序性，
但是 MySQL 优化器还是不能用 id 索引快速定位到 9999 这一行。所以，需要你在写 SQL 语句的时候，手动改写成 where id = 10000 -1 才可以。

select “10” > 9 返回的是 1，所以你就能确认 MySQL 里的转换规则了：在 MySQL 中，字符串和数字做比较的话，是将字符串转换成数字。


mysql> select * from tradelog where  CAST(tradid AS signed int) = 110717;

小表驱动大表：
 (这题目改成100万禾10000万比较好)
如果是考察语句写法，这两个表谁放前面都一样，优化器会调整顺序选择合适的驱动表；

如果是考察优化器怎么实现的，你可以这么想，每次在树搜索里面做一次查找都是log(n), 所以对比的是100*log(10000)和 10000*log(100)哪个小，
显然是前者，所以结论应该是让小表驱动大表。

Using index是覆盖索引
Using index condition 是索引下推

感觉要使用索引就不能“破坏”索引原有的顺序，这节的函数操作，隐式转换都“破坏”了原有的顺序。
前一节的select * from t where city in in (“杭州”," 苏州 ") order by name limit 100;
同样是破坏了 (city,name) 联合索引的递增顺序，类似的还有使用联合索引，一个字段DESC，一个ASC。

坏查询不一定是慢查询

你看到了，session A 先用 start transaction with consistent snapshot 命令启动了一个事务，之后 session B 才开始执行 update 语句。
session B 执行完 100 万次 update 语句后，id=1 这一行处于什么状态呢？你可以从图 16 中找到答案。
session B 更新完 100 万次，生成了 100 万个回滚日志 (undo log)。带 lock in share mode 的 SQL 语句，是当前读，因此会直接读到 1000001 这个结果，所以速度很快；
而 select * from t where id=1 这个语句，是一致性读，因此需要从 1000001 开始，依次执行 undo log，执行了 100 万次以后，才将 1 这个结果返回。
依次执行undo log才有结果返回

回滚最新一次的提交记录： git revert HEAD
回滚前一次的提交记录 ： git revert HEAD^


mysql> CREATE TABLE `table_a` (
  `id` int(11) NOT NULL,
  `b` varchar(10) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `b` (`b`)
) ENGINE=InnoDB;
有 100 万行数据，其中有 10 万行数据的 b 的值是’1234567890’

mysql> select * from table_a where b='1234567890abcd';
在传给引擎执行的时候，做了字符截断。因为引擎里面这个行只定义了长度是 10，所以只截了前 10 个字节，就是’1234567890’进去做匹配；
10万次回表，到server层判断不符合，记录将会过滤掉

RR隔离级别下，为保证binlog记录顺序，非索引更新会锁住全表记录，且事务结束前不会对不符合条件记录有逐步释放的过程。
为了提高数据库的并发能力，InnoDb 引入了行锁的概念

行锁和表锁一样，也分成两种类型：读锁和写锁。常见的增删改（INSERT、DELETE、UPDATE）语句会自动对操作的数据行加写锁，
查询的时候也可以明确指定锁的类型，SELECT ... LOCK IN SHARE MODE 语句加的是读锁，SELECT ... FOR UPDATE 语句加的是写锁。

行锁这个名字听起来像是这个锁加在某个数据行上，实际上这里要指出的是：在 MySQL 中，行锁是加在索引上的。

 B 树没有很好的伸缩性，它将多条数据都保存在节点里，
 如果数据中某个字段太长，一个 page 能容纳的数据量将受到限制，最坏的情况是一个 page 保存一条数据，这个时候 B 树退化成二叉树；
 另外 B 树无法修改字段最大长度，除非调整 page 大小，重建整个数据库。

 B+ 树里，内节点（非叶子节点）中不再保存数据，而只保存用于查找的 key，
 并且所有的叶子节点按顺序使用链表进行连接，这样可以大大的方便范围查询，只要先查到起始位置，然后按链表顺序查找，一直查到结束位置即可。

 1、当执行下面的 SQL 时（id 为 students 表的主键），我们要知道，InnoDb 存储引擎会在 id = 49 这个主键索引上加一把 X 锁。

 mysql> update students set score = 100 where id = 49;
 2、当执行下面的 SQL 时（name 为 students 表的二级索引），InnoDb 存储引擎会在 name = 'Tom' 这个索引上加一把 X 锁，
 同时会通过 name = 'Tom' 这个二级索引定位到 id = 49 这个主键索引，并在 id = 49 这个主键索引上加一把 X 锁。

 mysql> update students set score = 100 where name = 'Tom';

 3、像上面这样的 SQL 比较简单，只操作单条记录，如果要同时更新多条记录，加锁的过程又是什么样的呢？譬如下面的 SQL（假设 score 字段为二级索引）：

 mysql> update students set level = 3 where score >= 60;
 从图中可以看到当 UPDATE 语句被发给 MySQL 后，MySQL Server 会根据 WHERE 条件读取第一条满足条件的记录，
 然后 InnoDB 引擎会将第一条记录返回并加锁（current read），待 MySQL Server 收到这条加锁的记录之后，
 会再发起一个 UPDATE 请求，更新这条记录。一条记录操作完成，再读取下一条记录，直至没有满足条件的记录为止。
 因此，MySQL 在操作多条记录时 InnoDB 与 MySQL Server 的交互是一条一条进行的，
 加锁也是一条一条依次进行的，先对一条满足条件的记录加锁，返回给 MySQL Server，做一些 DML 操作，
 然后在读取下一条加锁，直至读取完毕。理解这一点，对我们后面分析复杂 SQL 语句的加锁过程将很有帮助。

根据锁的粒度可以把锁细分为表锁和行锁，行锁根据场景的不同又可以进一步细分，在 MySQL 的源码里，定义了四种类型的行锁，如下：

#define LOCK_TABLE  16  /* table lock */
#define LOCK_REC    32  /* record lock */

/* Precise modes */
#define LOCK_ORDINARY   0
#define LOCK_GAP    512
#define LOCK_REC_NOT_GAP 1024
#define LOCK_INSERT_INTENTION 2048
LOCK_ORDINARY：也称为 Next-Key Lock，锁一条记录及其之前的间隙，这是 RR 隔离级别用的最多的锁，从名字也能看出来；
LOCK_GAP：间隙锁，锁两个记录之间的 GAP，防止记录插入；
LOCK_REC_NOT_GAP：只锁记录；
LOCK_INSERT_INTENSION：插入意向 GAP 锁，插入记录时使用，是 LOCK_GAP 的一种特例。
这四种行锁将是理解并解决数据库死锁的关键，我们下面将深入研究这四种锁的特点。但是在介绍这四种锁之前，让我们再来看下 MySQL 下锁的模式。

锁类型和锁模式：
MySQL 将锁分成两类：锁类型（lock_type）和锁模式（lock_mode）。
锁类型就是上文中介绍的表锁和行锁两种类型，当然行锁还可以细分成记录锁和间隙锁等更细的类型，锁类型描述的锁的粒度，也可以说是把锁具体加在什么地方；
而锁模式描述的是到底加的是什么锁，譬如读锁或写锁。锁模式通常是和锁类型结合使用的，锁模式在 MySQL 的源码中定义如下：
/* Basic lock modes */
enum lock_mode {
    LOCK_IS = 0, /* intention shared */
    LOCK_IX,    /* intention exclusive */
    LOCK_S,     /* shared */
    LOCK_X,     /* exclusive */
    LOCK_AUTO_INC,  /* locks the auto-inc counter of a table in an exclusive mode*/
    ...
};
LOCK_IS：读意向锁；
LOCK_IX：写意向锁；
LOCK_S：读锁；
LOCK_X：写锁；
LOCK_AUTO_INC：自增锁；
将锁分为读锁和写锁主要是为了提高读的并发，如果不区分读写锁，那么数据库将没办法并发读，并发性将大大降低。
而 IS（读意向）、IX（写意向）只会应用在表锁上，方便表锁和行锁之间的冲突检测。LOCK_AUTO_INC 是一种特殊的表锁。下面依次进行介绍。

幻读指的是一个事务在前后两次查询同一个范围的时候，后一次查询看到了前一次查询没有看到的行。

对“幻读”做一个说明：在可重复读隔离级别下，普通的查询是快照读，是不会看到别的事务插入的数据的。
因此，幻读在“当前读”下才会出现。上面 session B 的修改结果，被 session A 之后的 select 语句用“当前读”看到，不能称为幻读。幻读仅专指“新插入的行”。

现在你知道了，产生幻读的原因是，行锁只能锁住行，但是新插入记录这个动作，要更新的是记录之间的“间隙”。
因此，为了解决幻读问题，InnoDB 只好引入新的锁，也就是间隙锁 (Gap Lock)。

这里 session B 并不会被堵住。因为表 t 里并没有 c=7 这个记录，因此 session A 加的是间隙锁 (5,10)。
而 session B 也是在这个间隙加的间隙锁。它们有共同的目标，即：保护这个间隙，不允许插入值。但，它们之间是不冲突的。
间隙锁和行锁合称 next-key lock，每个 next-key lock 是前开后闭区间。也就是说，我们的表 t 初始化以后，如果用 select * from t for update 要把整个表所有记录锁起来，
就形成了 7 个 next-key lock，分别是 (-∞,0]、(0,5]、(5,10]、(10,15]、(15,20]、(20, 25]、(25, +supremum]。

你看到了，其实都不需要用到后面的 update 语句，就已经形成死锁了。
我们按语句执行顺序来分析一下：session A 执行 select … for update 语句，
由于 id=9 这一行并不存在，因此会加上间隙锁 (5,10);session B 执行 select … for update 语句，
同样会加上间隙锁 (5,10)，间隙锁之间不会冲突，因此这个语句可以执行成功；
session B 试图插入一行 (9,9,9)，被 session A 的间隙锁挡住了，只好进入等待；session A 试图插入一行 (9,9,9)，被 session B 的间隙锁挡住了。
至此，两个 session 进入互相等待状态，形成死锁。
当然，InnoDB 的死锁检测马上就发现了这对死锁关系，让 session A 的 insert 语句报错返回了。

间隙锁的引入，可能会导致同样的语句锁住更大的范围，这其实是影响了并发度的。其实，这还只是一个简单的例子，在下一篇文章中我们还会碰到更多、更复杂的例子。

间隙锁是在可重复读隔离级别下才会生效的。所以，你如果把隔离级别设置为读提交的话，就没有间隙锁了。
但同时，你要解决可能出现的数据和日志不一致问题，需要把 binlog 格式设置为 row。这，也是现在不少公司使用的配置组合。

binlog日志有三种格式，分别为Statement,MiXED,以及ROW
show global variables like '%binlog_format%'   -> ROW
show global variables like '%isolation%'  -> transaction_isolation:READ-COMMITTED, tx_isolation:READ-COMMITTED

Statement：每一条会修改数据的sql都会记录在binlog中,优点是不需要记录每一行的变化，减少了binlog日志量，节约了IO，提高性能。
(相比row能节约多少性能与日志量，这个取决于应用的SQL情况，正常同一条记录修改或者插入row格式所产生的日志量还小于Statement产生的日志量，
    但是考虑到如果带条件的update操作，以及整表删除，alter表等操作，ROW格式会产生大量日志，
因此在考虑是否使用ROW格式日志时应该跟据应用的实际情况，其所产生的日志量会增加多少，以及带来的IO性能问题。)
由于记录的只是执行语句，为了这些语句能在slave上正确运行，因此还必须记录每条语句在执行的时候的一些相关信息，以保证所有语句能在slave得到和在master端执行时候相同的结果。
另外mysql 的复制,像一些特定函数功能，slave可与master上要保持一致会有很多相关问题(如sleep()函数， last_insert_id()，以及user-defined functions(udf)会出现问题).

Row：不记录sql语句上下文相关信息，仅保存哪条记录被修改。
rowlevel的日志内容会非常清楚的记录下每一行数据修改的细节
所有的执行的语句当记录到日志中的时候，都将以每行记录的修改来记录，这样可能会产生大量的日志内容,
比如一条update语句，修改多条记录，则binlog中每一条修改都会有记录，这样造成binlog日志量会很大，
特别是当执行alter table之类的语句的时候，由于表结构修改，每条记录都发生改变，那么该表每一条记录都会记录到日志中。

Mixedlevel: 是以上两种level的混合使用，一般的语句修改使用statment格式保存binlog，如一些函数，statement无法完成主从复制的操作，则采用row格式保存binlog